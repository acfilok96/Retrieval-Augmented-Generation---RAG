{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1."
      ],
      "metadata": {
        "id": "pHFtUOz91XDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-experimental langchain-google-genai langchain chromadb langchain-community pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPH0Z3SQySkA",
        "outputId": "4a88abc0-4dd1-453a-cd08-3b0a4f4a8c20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ucWVyS0ByGXi"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2."
      ],
      "metadata": {
        "id": "ngMPAgae1Y6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "mgefGRhYybPg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\",\n",
        "                                          google_api_key = GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "WIcWYxNUyd6-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3."
      ],
      "metadata": {
        "id": "cr1PuPkj1a1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/cGAN.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "KNtq6PPMCobi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDPr8iEzDBYy",
        "outputId": "8353957d-09d2-4c8f-9d89-283912a83923"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Conditional Generative Adversarial Nets\\nMehdi Mirza\\nD´epartement d’informatique et de recherche op ´erationnelle\\nUniversit ´e de Montr ´eal\\nMontr ´eal, QC H3C 3J7\\nmirzamom@iro.umontreal.ca\\nSimon Osindero\\nFlickr / Yahoo Inc.\\nSan Francisco, CA 94103\\nosindero@yahoo-inc.com\\nAbstract\\nGenerative Adversarial Nets [8] were recently introduced as a novel way to train\\ngenerative models. In this work we introduce the conditional version of generative\\nadversarial nets, which can be constructed by simply feeding the data, y, we wish\\nto condition on to both the generator and discriminator. We show that this model\\ncan generate MNIST digits conditioned on class labels. We also illustrate how\\nthis model could be used to learn a multi-modal model, and provide preliminary\\nexamples of an application to image tagging in which we demonstrate how this\\napproach can generate descriptive tags which are not part of training labels.\\n1 Introduction\\nGenerative adversarial nets were recently introduced as an alternative framework for training gen-\\nerative models in order to sidestep the difﬁculty of approximating many intractable probabilistic\\ncomputations.\\nAdversarial nets have the advantages that Markov chains are never needed, only backpropagation is\\nused to obtain gradients, no inference is required during learning, and a wide variety of factors and\\ninteractions can easily be incorporated into the model.\\nFurthermore, as demonstrated in [8], it can produce state of the art log-likelihood estimates and\\nrealistic samples.\\nIn an unconditioned generative model, there is no control on modes of the data being generated.\\nHowever, by conditioning the model on additional information it is possible to direct the data gener-\\nation process. Such conditioning could be based on class labels, on some part of data for inpainting\\nlike [5], or even on data from different modality.\\nIn this work we show how can we construct the conditional adversarial net. And for empirical results\\nwe demonstrate two set of experiment. One on MNIST digit data set conditioned on class labels and\\none on MIR Flickr 25,000 dataset [10] for multi-modal learning.\\n1arXiv:1411.1784v1  [cs.LG]  6 Nov 2014', metadata={'source': '/content/cGAN.pdf', 'page': 0}),\n",
              " Document(page_content='2 Related Work\\n2.1 Multi-modal Learning For Image Labelling\\nDespite the many recent successes of supervised neural networks (and convolutional networks in\\nparticular) [13, 17], it remains challenging to scale such models to accommodate an extremely large\\nnumber of predicted output categories. A second issue is that much of the work to date has focused\\non learning one-to-one mappings from input to output. However, many interesting problems are\\nmore naturally thought of as a probabilistic one-to-many mapping. For instance in the case of\\nimage labeling there may be many different tags that could appropriately applied to a given image,\\nand different (human) annotators may use different (but typically synonymous or related) terms to\\ndescribe the same image.\\nOne way to help address the ﬁrst issue is to leverage additional information from other modalities:\\nfor instance, by using natural language corpora to learn a vector representation for labels in which\\ngeometric relations are semantically meaningful. When making predictions in such spaces, we ben-\\neﬁt from the fact that when prediction errors we are still often ‘close’ to the truth (e.g. predicting\\n’table’ instead of ’chair’), and also from the fact that we can naturally make predictive generaliza-\\ntions to labels that were not seen during training time. Works such as [3] have shown that even a\\nsimple linear mapping from image feature-space to word-representation-space can yield improved\\nclassiﬁcation performance.\\nOne way to address the second problem is to use a conditional probabilistic generative model, the\\ninput is taken to be the conditioning variable and the one-to-many mapping is instantiated as a\\nconditional predictive distribution.\\n[16] take a similar approach to this problem, and train a multi-modal Deep Boltzmann Machine on\\nthe MIR Flickr 25,000 dataset as we do in this work.\\nAdditionally, in [12] the authors show how to train a supervised multi-modal neural language model,\\nand they are able to generate descriptive sentence for images.\\n3 Conditional Adversarial Nets\\n3.1 Generative Adversarial Nets\\nGenerative adversarial nets were recently introduced as a novel way to train a generative model.\\nThey consists of two ‘adversarial’ models: a generative model Gthat captures the data distribution,\\nand a discriminative model Dthat estimates the probability that a sample came from the training\\ndata rather than G. BothGandDcould be a non-linear mapping function, such as a multi-layer\\nperceptron.\\nTo learn a generator distribution pgover data data x, the generator builds a mapping function from\\na prior noise distribution pz(z)to data space as G(z;θg). And the discriminator, D(x;θd), outputs\\na single scalar representing the probability that xcame form training data rather than pg.\\nGandDare both trained simultaneously: we adjust parameters for Gto minimize log(1−D(G(z))\\nand adjust parameters for Dto minimize logD (X), as if they are following the two-player min-max\\ngame with value function V(G,D ):\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))]. (1)\\n3.2 Conditional Adversarial Nets\\nGenerative adversarial nets can be extended to a conditional model if both the generator and discrim-\\ninator are conditioned on some extra information y.ycould be any kind of auxiliary information,\\nsuch as class labels or data from other modalities. We can perform the conditioning by feeding y\\ninto the both the discriminator and generator as additional input layer.\\n2', metadata={'source': '/content/cGAN.pdf', 'page': 1}),\n",
              " Document(page_content='In the generator the prior input noise pz(z), andyare combined in joint hidden representation, and\\nthe adversarial training framework allows for considerable ﬂexibility in how this hidden representa-\\ntion is composed.1\\nIn the discriminator xandyare presented as inputs and to a discriminative function (embodied\\nagain by a MLP in this case).\\nThe objective function of a two-player minimax game would be as Eq 2\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x|y)] +Ez∼pz(z)[log(1−D(G(z|y)))]. (2)\\nFig 1 illustrates the structure of a simple conditional adversarial net.\\nFigure 1: Conditional adversarial net\\n4 Experimental Results\\n4.1 Unimodal\\nWe trained a conditional adversarial net on MNIST images conditioned on their class labels, encoded\\nas one-hot vectors.\\nIn the generator net, a noise prior zwith dimensionality 100 was drawn from a uniform distribution\\nwithin the unit hypercube. Both zandyare mapped to hidden layers with Rectiﬁed Linear Unit\\n(ReLu) activation [4, 11], with layer sizes 200 and 1000 respectively, before both being mapped to\\nsecond, combined hidden ReLu layer of dimensionality 1200. We then have a ﬁnal sigmoid unit\\nlayer as our output for generating the 784-dimensional MNIST samples.\\n1For now we simply have the conditioning input and prior noise as inputs to a single hidden layer of a MLP,\\nbut one could imagine using higher order interactions allowing for complex generation mechanisms that would\\nbe extremely difﬁcult to work with in a traditional generative framework.\\n3', metadata={'source': '/content/cGAN.pdf', 'page': 2}),\n",
              " Document(page_content='Model MNIST\\nDBN [1] 138±2\\nStacked CAE [1] 121±1.6\\nDeep GSN [2] 214±1.1\\nAdversarial nets 225±2\\nConditional adversarial nets 132±1.8\\nTable 1: Parzen window-based log-likelihood estimates for MNIST. We followed the same procedure as [8]\\nfor computing these values.\\nThe discriminator maps xto a maxout [6] layer with 240 units and 5 pieces, and yto a maxout layer\\nwith 50 units and 5 pieces. Both of the hidden layers mapped to a joint maxout layer with 240 units\\nand 4 pieces before being fed to the sigmoid layer. (The precise architecture of the discriminator\\nis not critical as long as it has sufﬁcient power; we have found that maxout units are typically well\\nsuited to the task.)\\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7. Dropout\\n[9] with probability of 0.5 was applied to both the generator and discriminator. And best estimate of\\nlog-likelihood on the validation set was used as stopping point.\\nTable 1 shows Gaussian Parzen window log-likelihood estimate for the MNIST dataset test data.\\n1000 samples were drawn from each 10 class and a Gaussian Parzen window was ﬁtted to these\\nsamples. We then estimate the log-likelihood of the test set using the Parzen window distribution.\\n(See [8] for more details of how this estimate is constructed.)\\nThe conditional adversarial net results that we present are comparable with some other network\\nbased, but are outperformed by several other approaches – including non-conditional adversarial\\nnets. We present these results more as a proof-of-concept than as demonstration of efﬁcacy, and\\nbelieve that with further exploration of hyper-parameter space and architecture that the conditional\\nmodel should match or exceed the non-conditional results.\\nFig 2 shows some of the generated samples. Each row is conditioned on one label and each column\\nis a different generated sample.\\nFigure 2: Generated MNIST digits, each row conditioned on one label\\n4.2 Multimodal\\nPhoto sites such as Flickr are a rich source of labeled data in the form of images and their associated\\nuser-generated metadata (UGM) — in particular user-tags.\\n4', metadata={'source': '/content/cGAN.pdf', 'page': 3}),\n",
              " Document(page_content='User-generated metadata differ from more ‘canonical’ image labelling schems in that they are typ-\\nically more descriptive, and are semantically much closer to how humans describe images with\\nnatural language rather than just identifying the objects present in an image. Another aspect of\\nUGM is that synoymy is prevalent and different users may use different vocabulary to describe the\\nsame concepts — consequently, having an efﬁcient way to normalize these labels becomes impor-\\ntant. Conceptual word embeddings [14] can be very useful here since related concepts end up being\\nrepresented by similar vectors.\\nIn this section we demonstrate automated tagging of images, with multi-label predictions, using con-\\nditional adversarial nets to generate a (possibly multi-modal) distribution of tag-vectors conditional\\non image features.\\nFor image features we pre-train a convolutional model similar to the one from [13] on the full\\nImageNet dataset with 21,000 labels [15]. We use the output of the last fully connected layer with\\n4096 units as image representations.\\nFor the world representation we ﬁrst gather a corpus of text from concatenation of user-tags, titles\\nand descriptions from YFCC100M2dataset metadata. After pre-processing and cleaning of the\\ntext we trained a skip-gram model [14] with word vector size of 200. And we omitted any word\\nappearing less than 200 times from the vocabulary, thereby ending up with a dictionary of size\\n247465.\\nWe keep the convolutional model and the language model ﬁxed during training of the adversarial\\nnet. And leave the experiments when we even backpropagate through these models as future work.\\nFor our experiments we use MIR Flickr 25,000 dataset [10], and extract the image and tags features\\nusing the convolutional model and language model we described above. Images without any tag\\nwere omitted from our experiments and annotations were treated as extra tags. The ﬁrst 150,000\\nexamples were used as training set. Images with multiple tags were repeated inside the training set\\nonce for each associated tag.\\nFor evaluation, we generate 100 samples for each image and ﬁnd top 20 closest words using cosine\\nsimilarity of vector representation of the words in the vocabulary to each sample. Then we select\\nthe top 10 most common words among all 100 samples. Table 4.2 shows some samples of the user\\nassigned tags and annotations along with the generated tags.\\nThe best working model’s generator receives Gaussian noise of size 100 as noise prior and maps it\\nto 500 dimension ReLu layer. And maps 4096 dimension image feature vector to 2000 dimension\\nReLu hidden layer. Both of these layers are mapped to a joint representation of 200 dimension linear\\nlayer which would output the generated word vectors.\\nThe discriminator is consisted of 500 and 1200 dimension ReLu hidden layers for word vectors and\\nimage features respectively and maxout layer with 1000 units and 3 pieces as the join layer which is\\nﬁnally fed to the one single sigmoid unit.\\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7. Dropout\\nwith probability of 0.5 was applied to both the generator and discriminator.\\nThe hyper-parameters and architectural choices were obtained by cross-validation and a mix of\\nrandom grid search and manual selection (albeit over a somewhat limited search space.)\\n5 Future Work\\nThe results shown in this paper are extremely preliminary, but they demonstrate the potential of\\nconditional adversarial nets and show promise for interesting and useful applications.\\nIn future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.', metadata={'source': '/content/cGAN.pdf', 'page': 4}),\n",
              " Document(page_content='In future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.\\n2Yahoo Flickr Creative Common 100M http://webscope.sandbox.yahoo.com/catalog.\\nphp?datatype=i&did=67 .\\n5', metadata={'source': '/content/cGAN.pdf', 'page': 4}),\n",
              " Document(page_content='User tags + annotations Generated tags\\nmontanha, trem, inverno,\\nfrio, people, male, plant\\nlife, tree, structures, trans-\\nport, cartaxi, passenger, line,\\ntransportation, railway\\nstation, passengers,\\nrailways, signals, rail,\\nrails\\nfood, raspberry, delicious,\\nhomemadechicken, fattening,\\ncooked, peanut, cream,\\ncookie, house made,\\nbread, biscuit, bakes\\nwater, rivercreek, lake, along, near,\\nriver, rocky, treeline, val-\\nley, woods, waters\\npeople, portrait, female,\\nbaby, indoorlove, people, posing, girl,\\nyoung, strangers, pretty,\\nwomen, happy, life\\nTable 2: Samples of generated tags\\nAlso, in the current experiments we only use each tag individually. But by using multiple tags at\\nthe same time (effectively posing generative problem as one of ‘set generation’) we hope to achieve\\nbetter results.\\nAnother obvious direction left for future work is to construct a joint training scheme to learn the\\nlanguage model. Works such as [12] has shown that we can learn a language model for suited for\\nthe speciﬁc task.\\nAcknowledgments\\nThis project was developed in Pylearn2 [7] framework, and we would like to thank Pylearn2 devel-\\nopers. We also like to thank Ian Goodfellow for helpful discussion during his afﬁliation at University\\nof Montreal. The authors gratefully acknowledge the support from the Vision & Machine Learning,\\nand Production Engineering teams at Flickr (in alphabetical order: Andrew Stadlen, Arel Cordero,\\nClayton Mellina, Cyprien Noel, Frank Liu, Gerry Pesavento, Huy Nguyen, Jack Culpepper, John\\nKo, Pierre Garrigues, Rob Hess, Stacey Svetlichnaya, Tobi Baumgartner, and Ye Lu).\\nReferences\\n[1] Bengio, Y ., Mesnil, G., Dauphin, Y ., and Rifai, S. (2013). Better mixing via deep representations. In\\nICML’2013 .\\n[2] Bengio, Y ., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative stochastic net-\\nworks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning\\n(ICML’14) .\\n6', metadata={'source': '/content/cGAN.pdf', 'page': 5}),\n",
              " Document(page_content='[3] Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al. (2013). Devise: A deep\\nvisual-semantic embedding model. In Advances in Neural Information Processing Systems , pages 2121–\\n2129.\\n[4] Glorot, X., Bordes, A., and Bengio, Y . (2011). Deep sparse rectiﬁer neural networks. In International\\nConference on Artiﬁcial Intelligence and Statistics , pages 315–323.\\n[5] Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y . (2013a). Multi-prediction deep boltzmann ma-\\nchines. In Advances in Neural Information Processing Systems , pages 548–556.\\n[6] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013b). Maxout networks.\\nInICML’2013 .\\n[7] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V ., Mirza, M., Pascanu, R., Bergstra, J.,\\nBastien, F., and Bengio, Y . (2013c). Pylearn2: a machine learning research library. arXiv preprint\\narXiv:1308.4214 .\\n[8] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\\nBengio, Y . (2014). Generative adversarial nets. In NIPS’2014 .\\n[9] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012). Improving\\nneural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.\\n[10] Huiskes, M. J. and Lew, M. S. (2008). The mir ﬂickr retrieval evaluation. In MIR ’08: Proceedings of the\\n2008 ACM International Conference on Multimedia Information Retrieval , New York, NY , USA. ACM.\\n[11] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y . (2009). What is the best multi-stage architecture\\nfor object recognition? In ICCV’09 .\\n[12] Kiros, R., Zemel, R., and Salakhutdinov, R. (2013). Multimodal neural language models. In Proc. NIPS\\nDeep Learning Workshop .\\n[13] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep convolutional\\nneural networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012) .\\n[14] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations in\\nvector space. In International Conference on Learning Representations: Workshops Track .\\n[15] Russakovsky, O. and Fei-Fei, L. (2010). Attribute learning in large-scale datasets. In European Confer-\\nence of Computer Vision (ECCV), International Workshop on Parts and Attributes , Crete, Greece.\\n[16] Srivastava, N. and Salakhutdinov, R. (2012). Multimodal learning with deep boltzmann machines. In\\nNIPS’2012 .\\n[17] Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., and Rabi-\\nnovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842 .\\n7', metadata={'source': '/content/cGAN.pdf', 'page': 6})]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = f\"\"\"\\n\\n\"\"\"\n",
        "\n",
        "for i in pages:\n",
        "  # print(i.page_content)\n",
        "  text1 = text1 + str(i.page_content)\n",
        "  # break"
      ],
      "metadata": {
        "id": "6cfIGrgTDOVt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "QbiLgyvhD1DI",
        "outputId": "c8a3e31a-21f1-43d4-c810-a6bb03f678d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nConditional Generative Adversarial Nets\\nMehdi Mirza\\nD´epartement d’informatique et de recherche op ´erationnelle\\nUniversit ´e de Montr ´eal\\nMontr ´eal, QC H3C 3J7\\nmirzamom@iro.umontreal.ca\\nSimon Osindero\\nFlickr / Yahoo Inc.\\nSan Francisco, CA 94103\\nosindero@yahoo-inc.com\\nAbstract\\nGenerative Adversarial Nets [8] were recently introduced as a novel way to train\\ngenerative models. In this work we introduce the conditional version of generative\\nadversarial nets, which can be constructed by simply feeding the data, y, we wish\\nto condition on to both the generator and discriminator. We show that this model\\ncan generate MNIST digits conditioned on class labels. We also illustrate how\\nthis model could be used to learn a multi-modal model, and provide preliminary\\nexamples of an application to image tagging in which we demonstrate how this\\napproach can generate descriptive tags which are not part of training labels.\\n1 Introduction\\nGenerative adversarial nets were recently introduced as an alternative framework for training gen-\\nerative models in order to sidestep the difﬁculty of approximating many intractable probabilistic\\ncomputations.\\nAdversarial nets have the advantages that Markov chains are never needed, only backpropagation is\\nused to obtain gradients, no inference is required during learning, and a wide variety of factors and\\ninteractions can easily be incorporated into the model.\\nFurthermore, as demonstrated in [8], it can produce state of the art log-likelihood estimates and\\nrealistic samples.\\nIn an unconditioned generative model, there is no control on modes of the data being generated.\\nHowever, by conditioning the model on additional information it is possible to direct the data gener-\\nation process. Such conditioning could be based on class labels, on some part of data for inpainting\\nlike [5], or even on data from different modality.\\nIn this work we show how can we construct the conditional adversarial net. And for empirical results\\nwe demonstrate two set of experiment. One on MNIST digit data set conditioned on class labels and\\none on MIR Flickr 25,000 dataset [10] for multi-modal learning.\\n1arXiv:1411.1784v1  [cs.LG]  6 Nov 20142 Related Work\\n2.1 Multi-modal Learning For Image Labelling\\nDespite the many recent successes of supervised neural networks (and convolutional networks in\\nparticular) [13, 17], it remains challenging to scale such models to accommodate an extremely large\\nnumber of predicted output categories. A second issue is that much of the work to date has focused\\non learning one-to-one mappings from input to output. However, many interesting problems are\\nmore naturally thought of as a probabilistic one-to-many mapping. For instance in the case of\\nimage labeling there may be many different tags that could appropriately applied to a given image,\\nand different (human) annotators may use different (but typically synonymous or related) terms to\\ndescribe the same image.\\nOne way to help address the ﬁrst issue is to leverage additional information from other modalities:\\nfor instance, by using natural language corpora to learn a vector representation for labels in which\\ngeometric relations are semantically meaningful. When making predictions in such spaces, we ben-\\neﬁt from the fact that when prediction errors we are still often ‘close’ to the truth (e.g. predicting\\n’table’ instead of ’chair’), and also from the fact that we can naturally make predictive generaliza-\\ntions to labels that were not seen during training time. Works such as [3] have shown that even a\\nsimple linear mapping from image feature-space to word-representation-space can yield improved\\nclassiﬁcation performance.\\nOne way to address the second problem is to use a conditional probabilistic generative model, the\\ninput is taken to be the conditioning variable and the one-to-many mapping is instantiated as a\\nconditional predictive distribution.\\n[16] take a similar approach to this problem, and train a multi-modal Deep Boltzmann Machine on\\nthe MIR Flickr 25,000 dataset as we do in this work.\\nAdditionally, in [12] the authors show how to train a supervised multi-modal neural language model,\\nand they are able to generate descriptive sentence for images.\\n3 Conditional Adversarial Nets\\n3.1 Generative Adversarial Nets\\nGenerative adversarial nets were recently introduced as a novel way to train a generative model.\\nThey consists of two ‘adversarial’ models: a generative model Gthat captures the data distribution,\\nand a discriminative model Dthat estimates the probability that a sample came from the training\\ndata rather than G. BothGandDcould be a non-linear mapping function, such as a multi-layer\\nperceptron.\\nTo learn a generator distribution pgover data data x, the generator builds a mapping function from\\na prior noise distribution pz(z)to data space as G(z;θg). And the discriminator, D(x;θd), outputs\\na single scalar representing the probability that xcame form training data rather than pg.\\nGandDare both trained simultaneously: we adjust parameters for Gto minimize log(1−D(G(z))\\nand adjust parameters for Dto minimize logD (X), as if they are following the two-player min-max\\ngame with value function V(G,D ):\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))]. (1)\\n3.2 Conditional Adversarial Nets\\nGenerative adversarial nets can be extended to a conditional model if both the generator and discrim-\\ninator are conditioned on some extra information y.ycould be any kind of auxiliary information,\\nsuch as class labels or data from other modalities. We can perform the conditioning by feeding y\\ninto the both the discriminator and generator as additional input layer.\\n2In the generator the prior input noise pz(z), andyare combined in joint hidden representation, and\\nthe adversarial training framework allows for considerable ﬂexibility in how this hidden representa-\\ntion is composed.1\\nIn the discriminator xandyare presented as inputs and to a discriminative function (embodied\\nagain by a MLP in this case).\\nThe objective function of a two-player minimax game would be as Eq 2\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x|y)] +Ez∼pz(z)[log(1−D(G(z|y)))]. (2)\\nFig 1 illustrates the structure of a simple conditional adversarial net.\\nFigure 1: Conditional adversarial net\\n4 Experimental Results\\n4.1 Unimodal\\nWe trained a conditional adversarial net on MNIST images conditioned on their class labels, encoded\\nas one-hot vectors.\\nIn the generator net, a noise prior zwith dimensionality 100 was drawn from a uniform distribution\\nwithin the unit hypercube. Both zandyare mapped to hidden layers with Rectiﬁed Linear Unit\\n(ReLu) activation [4, 11], with layer sizes 200 and 1000 respectively, before both being mapped to\\nsecond, combined hidden ReLu layer of dimensionality 1200. We then have a ﬁnal sigmoid unit\\nlayer as our output for generating the 784-dimensional MNIST samples.\\n1For now we simply have the conditioning input and prior noise as inputs to a single hidden layer of a MLP,\\nbut one could imagine using higher order interactions allowing for complex generation mechanisms that would\\nbe extremely difﬁcult to work with in a traditional generative framework.\\n3Model MNIST\\nDBN [1] 138±2\\nStacked CAE [1] 121±1.6\\nDeep GSN [2] 214±1.1\\nAdversarial nets 225±2\\nConditional adversarial nets 132±1.8\\nTable 1: Parzen window-based log-likelihood estimates for MNIST. We followed the same procedure as [8]\\nfor computing these values.\\nThe discriminator maps xto a maxout [6] layer with 240 units and 5 pieces, and yto a maxout layer\\nwith 50 units and 5 pieces. Both of the hidden layers mapped to a joint maxout layer with 240 units\\nand 4 pieces before being fed to the sigmoid layer. (The precise architecture of the discriminator\\nis not critical as long as it has sufﬁcient power; we have found that maxout units are typically well\\nsuited to the task.)\\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7. Dropout\\n[9] with probability of 0.5 was applied to both the generator and discriminator. And best estimate of\\nlog-likelihood on the validation set was used as stopping point.\\nTable 1 shows Gaussian Parzen window log-likelihood estimate for the MNIST dataset test data.\\n1000 samples were drawn from each 10 class and a Gaussian Parzen window was ﬁtted to these\\nsamples. We then estimate the log-likelihood of the test set using the Parzen window distribution.\\n(See [8] for more details of how this estimate is constructed.)\\nThe conditional adversarial net results that we present are comparable with some other network\\nbased, but are outperformed by several other approaches – including non-conditional adversarial\\nnets. We present these results more as a proof-of-concept than as demonstration of efﬁcacy, and\\nbelieve that with further exploration of hyper-parameter space and architecture that the conditional\\nmodel should match or exceed the non-conditional results.\\nFig 2 shows some of the generated samples. Each row is conditioned on one label and each column\\nis a different generated sample.\\nFigure 2: Generated MNIST digits, each row conditioned on one label\\n4.2 Multimodal\\nPhoto sites such as Flickr are a rich source of labeled data in the form of images and their associated\\nuser-generated metadata (UGM) — in particular user-tags.\\n4User-generated metadata differ from more ‘canonical’ image labelling schems in that they are typ-\\nically more descriptive, and are semantically much closer to how humans describe images with\\nnatural language rather than just identifying the objects present in an image. Another aspect of\\nUGM is that synoymy is prevalent and different users may use different vocabulary to describe the\\nsame concepts — consequently, having an efﬁcient way to normalize these labels becomes impor-\\ntant. Conceptual word embeddings [14] can be very useful here since related concepts end up being\\nrepresented by similar vectors.\\nIn this section we demonstrate automated tagging of images, with multi-label predictions, using con-\\nditional adversarial nets to generate a (possibly multi-modal) distribution of tag-vectors conditional\\non image features.\\nFor image features we pre-train a convolutional model similar to the one from [13] on the full\\nImageNet dataset with 21,000 labels [15]. We use the output of the last fully connected layer with\\n4096 units as image representations.\\nFor the world representation we ﬁrst gather a corpus of text from concatenation of user-tags, titles\\nand descriptions from YFCC100M2dataset metadata. After pre-processing and cleaning of the\\ntext we trained a skip-gram model [14] with word vector size of 200. And we omitted any word\\nappearing less than 200 times from the vocabulary, thereby ending up with a dictionary of size\\n247465.\\nWe keep the convolutional model and the language model ﬁxed during training of the adversarial\\nnet. And leave the experiments when we even backpropagate through these models as future work.\\nFor our experiments we use MIR Flickr 25,000 dataset [10], and extract the image and tags features\\nusing the convolutional model and language model we described above. Images without any tag\\nwere omitted from our experiments and annotations were treated as extra tags. The ﬁrst 150,000\\nexamples were used as training set. Images with multiple tags were repeated inside the training set\\nonce for each associated tag.\\nFor evaluation, we generate 100 samples for each image and ﬁnd top 20 closest words using cosine\\nsimilarity of vector representation of the words in the vocabulary to each sample. Then we select\\nthe top 10 most common words among all 100 samples. Table 4.2 shows some samples of the user\\nassigned tags and annotations along with the generated tags.\\nThe best working model’s generator receives Gaussian noise of size 100 as noise prior and maps it\\nto 500 dimension ReLu layer. And maps 4096 dimension image feature vector to 2000 dimension\\nReLu hidden layer. Both of these layers are mapped to a joint representation of 200 dimension linear\\nlayer which would output the generated word vectors.\\nThe discriminator is consisted of 500 and 1200 dimension ReLu hidden layers for word vectors and\\nimage features respectively and maxout layer with 1000 units and 3 pieces as the join layer which is\\nﬁnally fed to the one single sigmoid unit.\\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7. Dropout\\nwith probability of 0.5 was applied to both the generator and discriminator.\\nThe hyper-parameters and architectural choices were obtained by cross-validation and a mix of\\nrandom grid search and manual selection (albeit over a somewhat limited search space.)\\n5 Future Work\\nThe results shown in this paper are extremely preliminary, but they demonstrate the potential of\\nconditional adversarial nets and show promise for interesting and useful applications.\\nIn future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.In future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.\\n2Yahoo Flickr Creative Common 100M http://webscope.sandbox.yahoo.com/catalog.\\nphp?datatype=i&did=67 .\\n5User tags + annotations Generated tags\\nmontanha, trem, inverno,\\nfrio, people, male, plant\\nlife, tree, structures, trans-\\nport, cartaxi, passenger, line,\\ntransportation, railway\\nstation, passengers,\\nrailways, signals, rail,\\nrails\\nfood, raspberry, delicious,\\nhomemadechicken, fattening,\\ncooked, peanut, cream,\\ncookie, house made,\\nbread, biscuit, bakes\\nwater, rivercreek, lake, along, near,\\nriver, rocky, treeline, val-\\nley, woods, waters\\npeople, portrait, female,\\nbaby, indoorlove, people, posing, girl,\\nyoung, strangers, pretty,\\nwomen, happy, life\\nTable 2: Samples of generated tags\\nAlso, in the current experiments we only use each tag individually. But by using multiple tags at\\nthe same time (effectively posing generative problem as one of ‘set generation’) we hope to achieve\\nbetter results.\\nAnother obvious direction left for future work is to construct a joint training scheme to learn the\\nlanguage model. Works such as [12] has shown that we can learn a language model for suited for\\nthe speciﬁc task.\\nAcknowledgments\\nThis project was developed in Pylearn2 [7] framework, and we would like to thank Pylearn2 devel-\\nopers. We also like to thank Ian Goodfellow for helpful discussion during his afﬁliation at University\\nof Montreal. The authors gratefully acknowledge the support from the Vision & Machine Learning,\\nand Production Engineering teams at Flickr (in alphabetical order: Andrew Stadlen, Arel Cordero,\\nClayton Mellina, Cyprien Noel, Frank Liu, Gerry Pesavento, Huy Nguyen, Jack Culpepper, John\\nKo, Pierre Garrigues, Rob Hess, Stacey Svetlichnaya, Tobi Baumgartner, and Ye Lu).\\nReferences\\n[1] Bengio, Y ., Mesnil, G., Dauphin, Y ., and Rifai, S. (2013). Better mixing via deep representations. In\\nICML’2013 .\\n[2] Bengio, Y ., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative stochastic net-\\nworks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning\\n(ICML’14) .\\n6[3] Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al. (2013). Devise: A deep\\nvisual-semantic embedding model. In Advances in Neural Information Processing Systems , pages 2121–\\n2129.\\n[4] Glorot, X., Bordes, A., and Bengio, Y . (2011). Deep sparse rectiﬁer neural networks. In International\\nConference on Artiﬁcial Intelligence and Statistics , pages 315–323.\\n[5] Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y . (2013a). Multi-prediction deep boltzmann ma-\\nchines. In Advances in Neural Information Processing Systems , pages 548–556.\\n[6] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013b). Maxout networks.\\nInICML’2013 .\\n[7] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V ., Mirza, M., Pascanu, R., Bergstra, J.,\\nBastien, F., and Bengio, Y . (2013c). Pylearn2: a machine learning research library. arXiv preprint\\narXiv:1308.4214 .\\n[8] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\\nBengio, Y . (2014). Generative adversarial nets. In NIPS’2014 .\\n[9] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012). Improving\\nneural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580.\\n[10] Huiskes, M. J. and Lew, M. S. (2008). The mir ﬂickr retrieval evaluation. In MIR ’08: Proceedings of the\\n2008 ACM International Conference on Multimedia Information Retrieval , New York, NY , USA. ACM.\\n[11] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y . (2009). What is the best multi-stage architecture\\nfor object recognition? In ICCV’09 .\\n[12] Kiros, R., Zemel, R., and Salakhutdinov, R. (2013). Multimodal neural language models. In Proc. NIPS\\nDeep Learning Workshop .\\n[13] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep convolutional\\nneural networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012) .\\n[14] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations in\\nvector space. In International Conference on Learning Representations: Workshops Track .\\n[15] Russakovsky, O. and Fei-Fei, L. (2010). Attribute learning in large-scale datasets. In European Confer-\\nence of Computer Vision (ECCV), International Workshop on Parts and Attributes , Crete, Greece.\\n[16] Srivastava, N. and Salakhutdinov, R. (2012). Multimodal learning with deep boltzmann machines. In\\nNIPS’2012 .\\n[17] Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., and Rabi-\\nnovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842 .\\n7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text1"
      ],
      "metadata": {
        "id": "LH2j1yykD-ee"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = f\"\"\"The middle part of his career also saw him making important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. For much of the last phase of his academic life, Einstein worked on two endeavors that proved ultimately unsuccessful. Firstly, he advocated against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that 'God does not play dice'.\"\"\"\n",
        "# text"
      ],
      "metadata": {
        "id": "N4XLm_qU2bRw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1tmRzCV9kiv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = SemanticChunker(embeddings,\n",
        "                                breakpoint_threshold_type = \"percentile\" # \"standard_deviation\" # \"percentile\", \"interquartile\"\n",
        "                               )\n",
        "documents = text_splitter.create_documents([text])\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElFhW-bFyd3X",
        "outputId": "94983032-ebe5-49ba-b471-4928c1f5ec88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='\\n\\nConditional Generative Adversarial Nets\\nMehdi Mirza\\nD´epartement d’informatique et de recherche op ´erationnelle\\nUniversit ´e de Montr ´eal\\nMontr ´eal, QC H3C 3J7\\nmirzamom@iro.umontreal.ca\\nSimon Osindero\\nFlickr / Yahoo Inc. San Francisco, CA 94103\\nosindero@yahoo-inc.com\\nAbstract\\nGenerative Adversarial Nets [8] were recently introduced as a novel way to train\\ngenerative models. In this work we introduce the conditional version of generative\\nadversarial nets, which can be constructed by simply feeding the data, y, we wish\\nto condition on to both the generator and discriminator. We show that this model\\ncan generate MNIST digits conditioned on class labels. We also illustrate how\\nthis model could be used to learn a multi-modal model, and provide preliminary\\nexamples of an application to image tagging in which we demonstrate how this\\napproach can generate descriptive tags which are not part of training labels. 1 Introduction\\nGenerative adversarial nets were recently introduced as an alternative framework for training gen-\\nerative models in order to sidestep the difﬁculty of approximating many intractable probabilistic\\ncomputations. Adversarial nets have the advantages that Markov chains are never needed, only backpropagation is\\nused to obtain gradients, no inference is required during learning, and a wide variety of factors and\\ninteractions can easily be incorporated into the model. Furthermore, as demonstrated in [8], it can produce state of the art log-likelihood estimates and\\nrealistic samples. In an unconditioned generative model, there is no control on modes of the data being generated. However, by conditioning the model on additional information it is possible to direct the data gener-\\nation process. Such conditioning could be based on class labels, on some part of data for inpainting\\nlike [5], or even on data from different modality. In this work we show how can we construct the conditional adversarial net. And for empirical results\\nwe demonstrate two set of experiment. One on MNIST digit data set conditioned on class labels and\\none on MIR Flickr 25,000 dataset [10] for multi-modal learning. 1arXiv:1411.1784v1  [cs.LG]  6 Nov 20142 Related Work\\n2.1 Multi-modal Learning For Image Labelling\\nDespite the many recent successes of supervised neural networks (and convolutional networks in\\nparticular) [13, 17], it remains challenging to scale such models to accommodate an extremely large\\nnumber of predicted output categories. A second issue is that much of the work to date has focused\\non learning one-to-one mappings from input to output. However, many interesting problems are\\nmore naturally thought of as a probabilistic one-to-many mapping. For instance in the case of\\nimage labeling there may be many different tags that could appropriately applied to a given image,\\nand different (human) annotators may use different (but typically synonymous or related) terms to\\ndescribe the same image. One way to help address the ﬁrst issue is to leverage additional information from other modalities:\\nfor instance, by using natural language corpora to learn a vector representation for labels in which\\ngeometric relations are semantically meaningful. When making predictions in such spaces, we ben-\\neﬁt from the fact that when prediction errors we are still often ‘close’ to the truth (e.g. predicting\\n’table’ instead of ’chair’), and also from the fact that we can naturally make predictive generaliza-\\ntions to labels that were not seen during training time. Works such as [3] have shown that even a\\nsimple linear mapping from image feature-space to word-representation-space can yield improved\\nclassiﬁcation performance. One way to address the second problem is to use a conditional probabilistic generative model, the\\ninput is taken to be the conditioning variable and the one-to-many mapping is instantiated as a\\nconditional predictive distribution. [16] take a similar approach to this problem, and train a multi-modal Deep Boltzmann Machine on\\nthe MIR Flickr 25,000 dataset as we do in this work. Additionally, in [12] the authors show how to train a supervised multi-modal neural language model,\\nand they are able to generate descriptive sentence for images. 3 Conditional Adversarial Nets\\n3.1 Generative Adversarial Nets\\nGenerative adversarial nets were recently introduced as a novel way to train a generative model. They consists of two ‘adversarial’ models: a generative model Gthat captures the data distribution,\\nand a discriminative model Dthat estimates the probability that a sample came from the training\\ndata rather than G. BothGandDcould be a non-linear mapping function, such as a multi-layer\\nperceptron. To learn a generator distribution pgover data data x, the generator builds a mapping function from\\na prior noise distribution pz(z)to data space as G(z;θg). And the discriminator, D(x;θd), outputs\\na single scalar representing the probability that xcame form training data rather than pg. GandDare both trained simultaneously: we adjust parameters for Gto minimize log(1−D(G(z))\\nand adjust parameters for Dto minimize logD (X), as if they are following the two-player min-max\\ngame with value function V(G,D ):\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))]. (1)\\n3.2 Conditional Adversarial Nets\\nGenerative adversarial nets can be extended to a conditional model if both the generator and discrim-\\ninator are conditioned on some extra information y.ycould be any kind of auxiliary information,\\nsuch as class labels or data from other modalities. We can perform the conditioning by feeding y\\ninto the both the discriminator and generator as additional input layer. 2In the generator the prior input noise pz(z), andyare combined in joint hidden representation, and\\nthe adversarial training framework allows for considerable ﬂexibility in how this hidden representa-\\ntion is composed.1\\nIn the discriminator xandyare presented as inputs and to a discriminative function (embodied\\nagain by a MLP in this case). The objective function of a two-player minimax game would be as Eq 2\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x|y)] +Ez∼pz(z)[log(1−D(G(z|y)))]. (2)\\nFig 1 illustrates the structure of a simple conditional adversarial net. Figure 1: Conditional adversarial net\\n4 Experimental Results\\n4.1 Unimodal\\nWe trained a conditional adversarial net on MNIST images conditioned on their class labels, encoded\\nas one-hot vectors. In the generator net, a noise prior zwith dimensionality 100 was drawn from a uniform distribution\\nwithin the unit hypercube. Both zandyare mapped to hidden layers with Rectiﬁed Linear Unit\\n(ReLu) activation [4, 11], with layer sizes 200 and 1000 respectively, before both being mapped to\\nsecond, combined hidden ReLu layer of dimensionality 1200. We then have a ﬁnal sigmoid unit\\nlayer as our output for generating the 784-dimensional MNIST samples. 1For now we simply have the conditioning input and prior noise as inputs to a single hidden layer of a MLP,\\nbut one could imagine using higher order interactions allowing for complex generation mechanisms that would\\nbe extremely difﬁcult to work with in a traditional generative framework. 3Model MNIST\\nDBN [1] 138±2\\nStacked CAE [1] 121±1.6\\nDeep GSN [2] 214±1.1\\nAdversarial nets 225±2\\nConditional adversarial nets 132±1.8\\nTable 1: Parzen window-based log-likelihood estimates for MNIST. We followed the same procedure as [8]\\nfor computing these values.'),\n",
              " Document(page_content='The discriminator maps xto a maxout [6] layer with 240 units and 5 pieces, and yto a maxout layer\\nwith 50 units and 5 pieces. Both of the hidden layers mapped to a joint maxout layer with 240 units\\nand 4 pieces before being fed to the sigmoid layer. (The precise architecture of the discriminator\\nis not critical as long as it has sufﬁcient power; we have found that maxout units are typically well\\nsuited to the task.)\\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7. Dropout\\n[9] with probability of 0.5 was applied to both the generator and discriminator. And best estimate of\\nlog-likelihood on the validation set was used as stopping point. Table 1 shows Gaussian Parzen window log-likelihood estimate for the MNIST dataset test data. 1000 samples were drawn from each 10 class and a Gaussian Parzen window was ﬁtted to these\\nsamples. We then estimate the log-likelihood of the test set using the Parzen window distribution. (See [8] for more details of how this estimate is constructed.)\\nThe conditional adversarial net results that we present are comparable with some other network\\nbased, but are outperformed by several other approaches – including non-conditional adversarial\\nnets. We present these results more as a proof-of-concept than as demonstration of efﬁcacy, and\\nbelieve that with further exploration of hyper-parameter space and architecture that the conditional\\nmodel should match or exceed the non-conditional results. Fig 2 shows some of the generated samples.'),\n",
              " Document(page_content='Each row is conditioned on one label and each column\\nis a different generated sample. Figure 2: Generated MNIST digits, each row conditioned on one label\\n4.2 Multimodal\\nPhoto sites such as Flickr are a rich source of labeled data in the form of images and their associated\\nuser-generated metadata (UGM) — in particular user-tags. 4User-generated metadata differ from more ‘canonical’ image labelling schems in that they are typ-\\nically more descriptive, and are semantically much closer to how humans describe images with\\nnatural language rather than just identifying the objects present in an image. Another aspect of\\nUGM is that synoymy is prevalent and different users may use different vocabulary to describe the\\nsame concepts — consequently, having an efﬁcient way to normalize these labels becomes impor-\\ntant. Conceptual word embeddings [14] can be very useful here since related concepts end up being\\nrepresented by similar vectors. In this section we demonstrate automated tagging of images, with multi-label predictions, using con-\\nditional adversarial nets to generate a (possibly multi-modal) distribution of tag-vectors conditional\\non image features. For image features we pre-train a convolutional model similar to the one from [13] on the full\\nImageNet dataset with 21,000 labels [15].'),\n",
              " Document(page_content='We use the output of the last fully connected layer with\\n4096 units as image representations. For the world representation we ﬁrst gather a corpus of text from concatenation of user-tags, titles\\nand descriptions from YFCC100M2dataset metadata. After pre-processing and cleaning of the\\ntext we trained a skip-gram model [14] with word vector size of 200. And we omitted any word\\nappearing less than 200 times from the vocabulary, thereby ending up with a dictionary of size\\n247465. We keep the convolutional model and the language model ﬁxed during training of the adversarial\\nnet. And leave the experiments when we even backpropagate through these models as future work. For our experiments we use MIR Flickr 25,000 dataset [10], and extract the image and tags features\\nusing the convolutional model and language model we described above. Images without any tag\\nwere omitted from our experiments and annotations were treated as extra tags. The ﬁrst 150,000\\nexamples were used as training set. Images with multiple tags were repeated inside the training set\\nonce for each associated tag. For evaluation, we generate 100 samples for each image and ﬁnd top 20 closest words using cosine\\nsimilarity of vector representation of the words in the vocabulary to each sample. Then we select\\nthe top 10 most common words among all 100 samples. Table 4.2 shows some samples of the user\\nassigned tags and annotations along with the generated tags. The best working model’s generator receives Gaussian noise of size 100 as noise prior and maps it\\nto 500 dimension ReLu layer. And maps 4096 dimension image feature vector to 2000 dimension\\nReLu hidden layer. Both of these layers are mapped to a joint representation of 200 dimension linear\\nlayer which would output the generated word vectors. The discriminator is consisted of 500 and 1200 dimension ReLu hidden layers for word vectors and\\nimage features respectively and maxout layer with 1000 units and 3 pieces as the join layer which is\\nﬁnally fed to the one single sigmoid unit. The model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7.'),\n",
              " Document(page_content='Dropout\\nwith probability of 0.5 was applied to both the generator and discriminator. The hyper-parameters and architectural choices were obtained by cross-validation and a mix of\\nrandom grid search and manual selection (albeit over a somewhat limited search space.)\\n5 Future Work\\nThe results shown in this paper are extremely preliminary, but they demonstrate the potential of\\nconditional adversarial nets and show promise for interesting and useful applications. In future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.In future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.'),\n",
              " Document(page_content='2Yahoo Flickr Creative Common 100M http://webscope.sandbox.yahoo.com/catalog.'),\n",
              " Document(page_content='php?datatype=i&did=67 . 5User tags + annotations Generated tags\\nmontanha, trem, inverno,\\nfrio, people, male, plant\\nlife, tree, structures, trans-\\nport, cartaxi, passenger, line,\\ntransportation, railway\\nstation, passengers,\\nrailways, signals, rail,\\nrails\\nfood, raspberry, delicious,\\nhomemadechicken, fattening,\\ncooked, peanut, cream,\\ncookie, house made,\\nbread, biscuit, bakes\\nwater, rivercreek, lake, along, near,\\nriver, rocky, treeline, val-\\nley, woods, waters\\npeople, portrait, female,\\nbaby, indoorlove, people, posing, girl,\\nyoung, strangers, pretty,\\nwomen, happy, life\\nTable 2: Samples of generated tags\\nAlso, in the current experiments we only use each tag individually. But by using multiple tags at\\nthe same time (effectively posing generative problem as one of ‘set generation’) we hope to achieve\\nbetter results.'),\n",
              " Document(page_content='Another obvious direction left for future work is to construct a joint training scheme to learn the\\nlanguage model. Works such as [12] has shown that we can learn a language model for suited for\\nthe speciﬁc task. Acknowledgments\\nThis project was developed in Pylearn2 [7] framework, and we would like to thank Pylearn2 devel-\\nopers. We also like to thank Ian Goodfellow for helpful discussion during his afﬁliation at University\\nof Montreal. The authors gratefully acknowledge the support from the Vision & Machine Learning,\\nand Production Engineering teams at Flickr (in alphabetical order: Andrew Stadlen, Arel Cordero,\\nClayton Mellina, Cyprien Noel, Frank Liu, Gerry Pesavento, Huy Nguyen, Jack Culpepper, John\\nKo, Pierre Garrigues, Rob Hess, Stacey Svetlichnaya, Tobi Baumgartner, and Ye Lu). References\\n[1] Bengio, Y ., Mesnil, G., Dauphin, Y ., and Rifai, S. (2013). Better mixing via deep representations. In\\nICML’2013 . [2] Bengio, Y ., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative stochastic net-\\nworks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning\\n(ICML’14) . 6[3] Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al. (2013). Devise: A deep\\nvisual-semantic embedding model. In Advances in Neural Information Processing Systems , pages 2121–\\n2129. [4] Glorot, X., Bordes, A., and Bengio, Y . (2011). Deep sparse rectiﬁer neural networks. In International\\nConference on Artiﬁcial Intelligence and Statistics , pages 315–323. [5] Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y . (2013a). Multi-prediction deep boltzmann ma-\\nchines. In Advances in Neural Information Processing Systems , pages 548–556. [6] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013b). Maxout networks. InICML’2013 . [7] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V ., Mirza, M., Pascanu, R., Bergstra, J.,\\nBastien, F., and Bengio, Y . (2013c). Pylearn2: a machine learning research library. arXiv preprint\\narXiv:1308.4214 . [8] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\\nBengio, Y . (2014). Generative adversarial nets. In NIPS’2014 . [9] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012). Improving\\nneural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580. [10] Huiskes, M. J. and Lew, M. S.'),\n",
              " Document(page_content='(2008). The mir ﬂickr retrieval evaluation. In MIR ’08: Proceedings of the\\n2008 ACM International Conference on Multimedia Information Retrieval , New York, NY , USA. ACM. [11] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y . (2009). What is the best multi-stage architecture\\nfor object recognition? In ICCV’09 . [12] Kiros, R., Zemel, R., and Salakhutdinov, R. (2013). Multimodal neural language models. In Proc. NIPS\\nDeep Learning Workshop . [13] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep convolutional\\nneural networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012) . [14] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations in\\nvector space. In International Conference on Learning Representations: Workshops Track . [15] Russakovsky, O. and Fei-Fei, L. (2010). Attribute learning in large-scale datasets. In European Confer-\\nence of Computer Vision (ECCV), International Workshop on Parts and Attributes , Crete, Greece. [16] Srivastava, N. and Salakhutdinov, R.'),\n",
              " Document(page_content='(2012). Multimodal learning with deep boltzmann machines. In\\nNIPS’2012 . [17] Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., and Rabi-\\nnovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842 . 7')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model = \"gemini-pro\",\n",
        "                             google_api_key = GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "nAe1QLkB4DFv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "y0QQMYmU7FI9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_list = []\n",
        "for i in documents:\n",
        "  txt = str(i.page_content)\n",
        "  # prompt = f\"\"\"Summarize the given text carefully within 30 words.\n",
        "  #           text: {txt}\n",
        "  #           \"\"\"\n",
        "  prompt = f\"\"\"Your task is to generate a short summary of a given phrase.\n",
        "                 Phrase: ```{txt}``` \"\"\"\n",
        "  txt_dict = {}\n",
        "  txt_dict[\"text\"] = str(i.page_content)\n",
        "  txt_dict[\"summary\"] = str(llm.invoke(prompt).content)\n",
        "  txt_list.append(txt_dict)"
      ],
      "metadata": {
        "id": "VRcA4t1i297t"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "for i in txt_list:\n",
        "  temp = Document(page_content = str(i[\"text\"]),\n",
        "                  metadata = {\"summary\" : str(i[\"summary\"])})\n",
        "  docs.append(temp)"
      ],
      "metadata": {
        "id": "TzWyAXOI5_9X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T21yk_j99Ybd",
        "outputId": "7c46ae08-7f5a-4d2c-fbbe-7d86997696ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='\\n\\nConditional Generative Adversarial Nets\\nMehdi Mirza\\nD´epartement d’informatique et de recherche op ´erationnelle\\nUniversit ´e de Montr ´eal\\nMontr ´eal, QC H3C 3J7\\nmirzamom@iro.umontreal.ca\\nSimon Osindero\\nFlickr / Yahoo Inc. San Francisco, CA 94103\\nosindero@yahoo-inc.com\\nAbstract\\nGenerative Adversarial Nets [8] were recently introduced as a novel way to train\\ngenerative models. In this work we introduce the conditional version of generative\\nadversarial nets, which can be constructed by simply feeding the data, y, we wish\\nto condition on to both the generator and discriminator. We show that this model\\ncan generate MNIST digits conditioned on class labels. We also illustrate how\\nthis model could be used to learn a multi-modal model, and provide preliminary\\nexamples of an application to image tagging in which we demonstrate how this\\napproach can generate descriptive tags which are not part of training labels. 1 Introduction\\nGenerative adversarial nets were recently introduced as an alternative framework for training gen-\\nerative models in order to sidestep the difﬁculty of approximating many intractable probabilistic\\ncomputations. Adversarial nets have the advantages that Markov chains are never needed, only backpropagation is\\nused to obtain gradients, no inference is required during learning, and a wide variety of factors and\\ninteractions can easily be incorporated into the model. Furthermore, as demonstrated in [8], it can produce state of the art log-likelihood estimates and\\nrealistic samples. In an unconditioned generative model, there is no control on modes of the data being generated. However, by conditioning the model on additional information it is possible to direct the data gener-\\nation process. Such conditioning could be based on class labels, on some part of data for inpainting\\nlike [5], or even on data from different modality. In this work we show how can we construct the conditional adversarial net. And for empirical results\\nwe demonstrate two set of experiment. One on MNIST digit data set conditioned on class labels and\\none on MIR Flickr 25,000 dataset [10] for multi-modal learning. 1arXiv:1411.1784v1  [cs.LG]  6 Nov 20142 Related Work\\n2.1 Multi-modal Learning For Image Labelling\\nDespite the many recent successes of supervised neural networks (and convolutional networks in\\nparticular) [13, 17], it remains challenging to scale such models to accommodate an extremely large\\nnumber of predicted output categories. A second issue is that much of the work to date has focused\\non learning one-to-one mappings from input to output. However, many interesting problems are\\nmore naturally thought of as a probabilistic one-to-many mapping. For instance in the case of\\nimage labeling there may be many different tags that could appropriately applied to a given image,\\nand different (human) annotators may use different (but typically synonymous or related) terms to\\ndescribe the same image. One way to help address the ﬁrst issue is to leverage additional information from other modalities:\\nfor instance, by using natural language corpora to learn a vector representation for labels in which\\ngeometric relations are semantically meaningful. When making predictions in such spaces, we ben-\\neﬁt from the fact that when prediction errors we are still often ‘close’ to the truth (e.g. predicting\\n’table’ instead of ’chair’), and also from the fact that we can naturally make predictive generaliza-\\ntions to labels that were not seen during training time. Works such as [3] have shown that even a\\nsimple linear mapping from image feature-space to word-representation-space can yield improved\\nclassiﬁcation performance. One way to address the second problem is to use a conditional probabilistic generative model, the\\ninput is taken to be the conditioning variable and the one-to-many mapping is instantiated as a\\nconditional predictive distribution. [16] take a similar approach to this problem, and train a multi-modal Deep Boltzmann Machine on\\nthe MIR Flickr 25,000 dataset as we do in this work. Additionally, in [12] the authors show how to train a supervised multi-modal neural language model,\\nand they are able to generate descriptive sentence for images. 3 Conditional Adversarial Nets\\n3.1 Generative Adversarial Nets\\nGenerative adversarial nets were recently introduced as a novel way to train a generative model. They consists of two ‘adversarial’ models: a generative model Gthat captures the data distribution,\\nand a discriminative model Dthat estimates the probability that a sample came from the training\\ndata rather than G. BothGandDcould be a non-linear mapping function, such as a multi-layer\\nperceptron. To learn a generator distribution pgover data data x, the generator builds a mapping function from\\na prior noise distribution pz(z)to data space as G(z;θg). And the discriminator, D(x;θd), outputs\\na single scalar representing the probability that xcame form training data rather than pg. GandDare both trained simultaneously: we adjust parameters for Gto minimize log(1−D(G(z))\\nand adjust parameters for Dto minimize logD (X), as if they are following the two-player min-max\\ngame with value function V(G,D ):\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))]. (1)\\n3.2 Conditional Adversarial Nets\\nGenerative adversarial nets can be extended to a conditional model if both the generator and discrim-\\ninator are conditioned on some extra information y.ycould be any kind of auxiliary information,\\nsuch as class labels or data from other modalities. We can perform the conditioning by feeding y\\ninto the both the discriminator and generator as additional input layer. 2In the generator the prior input noise pz(z), andyare combined in joint hidden representation, and\\nthe adversarial training framework allows for considerable ﬂexibility in how this hidden representa-\\ntion is composed.1\\nIn the discriminator xandyare presented as inputs and to a discriminative function (embodied\\nagain by a MLP in this case). The objective function of a two-player minimax game would be as Eq 2\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x|y)] +Ez∼pz(z)[log(1−D(G(z|y)))]. (2)\\nFig 1 illustrates the structure of a simple conditional adversarial net. Figure 1: Conditional adversarial net\\n4 Experimental Results\\n4.1 Unimodal\\nWe trained a conditional adversarial net on MNIST images conditioned on their class labels, encoded\\nas one-hot vectors. In the generator net, a noise prior zwith dimensionality 100 was drawn from a uniform distribution\\nwithin the unit hypercube. Both zandyare mapped to hidden layers with Rectiﬁed Linear Unit\\n(ReLu) activation [4, 11], with layer sizes 200 and 1000 respectively, before both being mapped to\\nsecond, combined hidden ReLu layer of dimensionality 1200. We then have a ﬁnal sigmoid unit\\nlayer as our output for generating the 784-dimensional MNIST samples. 1For now we simply have the conditioning input and prior noise as inputs to a single hidden layer of a MLP,\\nbut one could imagine using higher order interactions allowing for complex generation mechanisms that would\\nbe extremely difﬁcult to work with in a traditional generative framework. 3Model MNIST\\nDBN [1] 138±2\\nStacked CAE [1] 121±1.6\\nDeep GSN [2] 214±1.1\\nAdversarial nets 225±2\\nConditional adversarial nets 132±1.8\\nTable 1: Parzen window-based log-likelihood estimates for MNIST. We followed the same procedure as [8]\\nfor computing these values.', metadata={'summary': 'Conditional Generative Adversarial Nets (Conditional GANs) extend Generative Adversarial Nets by conditioning both the generator and discriminator on additional information (e.g., class labels, data from other modalities). This enables directed data generation based on the conditioning information, opening up applications in multi-modal learning, image tagging, and other domains where control over the generated data is desired. Conditional GANs have shown promising results in generating data conditioned on class labels and learning multi-modal models, demonstrating the flexibility and potential of adversarial networks in conditional generation tasks.'}),\n",
              " Document(page_content='The discriminator maps xto a maxout [6] layer with 240 units and 5 pieces, and yto a maxout layer\\nwith 50 units and 5 pieces. Both of the hidden layers mapped to a joint maxout layer with 240 units\\nand 4 pieces before being fed to the sigmoid layer. (The precise architecture of the discriminator\\nis not critical as long as it has sufﬁcient power; we have found that maxout units are typically well\\nsuited to the task.)\\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7. Dropout\\n[9] with probability of 0.5 was applied to both the generator and discriminator. And best estimate of\\nlog-likelihood on the validation set was used as stopping point. Table 1 shows Gaussian Parzen window log-likelihood estimate for the MNIST dataset test data. 1000 samples were drawn from each 10 class and a Gaussian Parzen window was ﬁtted to these\\nsamples. We then estimate the log-likelihood of the test set using the Parzen window distribution. (See [8] for more details of how this estimate is constructed.)\\nThe conditional adversarial net results that we present are comparable with some other network\\nbased, but are outperformed by several other approaches – including non-conditional adversarial\\nnets. We present these results more as a proof-of-concept than as demonstration of efﬁcacy, and\\nbelieve that with further exploration of hyper-parameter space and architecture that the conditional\\nmodel should match or exceed the non-conditional results. Fig 2 shows some of the generated samples.', metadata={'summary': 'The discriminator network in a conditional adversarial network consists of multiple maxout layers with varying units and pieces, followed by a joint maxout layer and a sigmoid layer. The model is trained using stochastic gradient descent with various optimization techniques such as momentum, learning rate decay, and dropout. The Gaussian Parzen window log-likelihood estimate is used as a stopping point for training. The conditional adversarial net results are comparable to other network-based methods but have potential for further improvement with hyperparameter optimization and architectural exploration.'}),\n",
              " Document(page_content='Each row is conditioned on one label and each column\\nis a different generated sample. Figure 2: Generated MNIST digits, each row conditioned on one label\\n4.2 Multimodal\\nPhoto sites such as Flickr are a rich source of labeled data in the form of images and their associated\\nuser-generated metadata (UGM) — in particular user-tags. 4User-generated metadata differ from more ‘canonical’ image labelling schems in that they are typ-\\nically more descriptive, and are semantically much closer to how humans describe images with\\nnatural language rather than just identifying the objects present in an image. Another aspect of\\nUGM is that synoymy is prevalent and different users may use different vocabulary to describe the\\nsame concepts — consequently, having an efﬁcient way to normalize these labels becomes impor-\\ntant. Conceptual word embeddings [14] can be very useful here since related concepts end up being\\nrepresented by similar vectors. In this section we demonstrate automated tagging of images, with multi-label predictions, using con-\\nditional adversarial nets to generate a (possibly multi-modal) distribution of tag-vectors conditional\\non image features. For image features we pre-train a convolutional model similar to the one from [13] on the full\\nImageNet dataset with 21,000 labels [15].', metadata={'summary': 'In conditional adversarial networks, each row represents a label, and each column is a generated sample. The network generates MNIST digits, with each row conditioned on a specific label. The model also handles multimodal data, such as user-generated metadata for images, where multiple labels may describe the same concept.'}),\n",
              " Document(page_content='We use the output of the last fully connected layer with\\n4096 units as image representations. For the world representation we ﬁrst gather a corpus of text from concatenation of user-tags, titles\\nand descriptions from YFCC100M2dataset metadata. After pre-processing and cleaning of the\\ntext we trained a skip-gram model [14] with word vector size of 200. And we omitted any word\\nappearing less than 200 times from the vocabulary, thereby ending up with a dictionary of size\\n247465. We keep the convolutional model and the language model ﬁxed during training of the adversarial\\nnet. And leave the experiments when we even backpropagate through these models as future work. For our experiments we use MIR Flickr 25,000 dataset [10], and extract the image and tags features\\nusing the convolutional model and language model we described above. Images without any tag\\nwere omitted from our experiments and annotations were treated as extra tags. The ﬁrst 150,000\\nexamples were used as training set. Images with multiple tags were repeated inside the training set\\nonce for each associated tag. For evaluation, we generate 100 samples for each image and ﬁnd top 20 closest words using cosine\\nsimilarity of vector representation of the words in the vocabulary to each sample. Then we select\\nthe top 10 most common words among all 100 samples. Table 4.2 shows some samples of the user\\nassigned tags and annotations along with the generated tags. The best working model’s generator receives Gaussian noise of size 100 as noise prior and maps it\\nto 500 dimension ReLu layer. And maps 4096 dimension image feature vector to 2000 dimension\\nReLu hidden layer. Both of these layers are mapped to a joint representation of 200 dimension linear\\nlayer which would output the generated word vectors. The discriminator is consisted of 500 and 1200 dimension ReLu hidden layers for word vectors and\\nimage features respectively and maxout layer with 1000 units and 3 pieces as the join layer which is\\nﬁnally fed to the one single sigmoid unit. The model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7.', metadata={'summary': 'This work uses a skip-gram model to represent text from image metadata as word vectors. Image representations are obtained from a convolutional model. An adversarial network is trained to map Gaussian noise and image features to word vectors. The model generates tags that are similar to user-assigned tags.'}),\n",
              " Document(page_content='Dropout\\nwith probability of 0.5 was applied to both the generator and discriminator. The hyper-parameters and architectural choices were obtained by cross-validation and a mix of\\nrandom grid search and manual selection (albeit over a somewhat limited search space.)\\n5 Future Work\\nThe results shown in this paper are extremely preliminary, but they demonstrate the potential of\\nconditional adversarial nets and show promise for interesting and useful applications. In future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.In future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.', metadata={'summary': 'Dropout with a probability of 0.5 was applied to both the generator and discriminator networks. Hyperparameters and architectural choices were obtained through cross-validation and a combination of random grid search and manual selection.'}),\n",
              " Document(page_content='2Yahoo Flickr Creative Common 100M http://webscope.sandbox.yahoo.com/catalog.', metadata={'summary': 'Yahoo Flickr Creative Common 100M is a dataset of 100 million Creative Common licensed images from Flickr. It is hosted by Yahoo Webscope and can be accessed through its catalog.'}),\n",
              " Document(page_content='php?datatype=i&did=67 . 5User tags + annotations Generated tags\\nmontanha, trem, inverno,\\nfrio, people, male, plant\\nlife, tree, structures, trans-\\nport, cartaxi, passenger, line,\\ntransportation, railway\\nstation, passengers,\\nrailways, signals, rail,\\nrails\\nfood, raspberry, delicious,\\nhomemadechicken, fattening,\\ncooked, peanut, cream,\\ncookie, house made,\\nbread, biscuit, bakes\\nwater, rivercreek, lake, along, near,\\nriver, rocky, treeline, val-\\nley, woods, waters\\npeople, portrait, female,\\nbaby, indoorlove, people, posing, girl,\\nyoung, strangers, pretty,\\nwomen, happy, life\\nTable 2: Samples of generated tags\\nAlso, in the current experiments we only use each tag individually. But by using multiple tags at\\nthe same time (effectively posing generative problem as one of ‘set generation’) we hope to achieve\\nbetter results.', metadata={'summary': 'This phrase describes a user-generated tagging system for images, where users can add tags to images to describe their content. The system then uses these tags to generate additional tags that are relevant to the image. The phrase also mentions that the system is currently only using each tag individually, but that the researchers hope to achieve better results by using multiple tags at the same time.'}),\n",
              " Document(page_content='Another obvious direction left for future work is to construct a joint training scheme to learn the\\nlanguage model. Works such as [12] has shown that we can learn a language model for suited for\\nthe speciﬁc task. Acknowledgments\\nThis project was developed in Pylearn2 [7] framework, and we would like to thank Pylearn2 devel-\\nopers. We also like to thank Ian Goodfellow for helpful discussion during his afﬁliation at University\\nof Montreal. The authors gratefully acknowledge the support from the Vision & Machine Learning,\\nand Production Engineering teams at Flickr (in alphabetical order: Andrew Stadlen, Arel Cordero,\\nClayton Mellina, Cyprien Noel, Frank Liu, Gerry Pesavento, Huy Nguyen, Jack Culpepper, John\\nKo, Pierre Garrigues, Rob Hess, Stacey Svetlichnaya, Tobi Baumgartner, and Ye Lu). References\\n[1] Bengio, Y ., Mesnil, G., Dauphin, Y ., and Rifai, S. (2013). Better mixing via deep representations. In\\nICML’2013 . [2] Bengio, Y ., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative stochastic net-\\nworks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning\\n(ICML’14) . 6[3] Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al. (2013). Devise: A deep\\nvisual-semantic embedding model. In Advances in Neural Information Processing Systems , pages 2121–\\n2129. [4] Glorot, X., Bordes, A., and Bengio, Y . (2011). Deep sparse rectiﬁer neural networks. In International\\nConference on Artiﬁcial Intelligence and Statistics , pages 315–323. [5] Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y . (2013a). Multi-prediction deep boltzmann ma-\\nchines. In Advances in Neural Information Processing Systems , pages 548–556. [6] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013b). Maxout networks. InICML’2013 . [7] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V ., Mirza, M., Pascanu, R., Bergstra, J.,\\nBastien, F., and Bengio, Y . (2013c). Pylearn2: a machine learning research library. arXiv preprint\\narXiv:1308.4214 . [8] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\\nBengio, Y . (2014). Generative adversarial nets. In NIPS’2014 . [9] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012). Improving\\nneural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580. [10] Huiskes, M. J. and Lew, M. S.', metadata={'summary': 'A potential area for further research is developing a joint training method for a language model. Previous studies have demonstrated the feasibility of creating task-specific language models.'}),\n",
              " Document(page_content='(2008). The mir ﬂickr retrieval evaluation. In MIR ’08: Proceedings of the\\n2008 ACM International Conference on Multimedia Information Retrieval , New York, NY , USA. ACM. [11] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y . (2009). What is the best multi-stage architecture\\nfor object recognition? In ICCV’09 . [12] Kiros, R., Zemel, R., and Salakhutdinov, R. (2013). Multimodal neural language models. In Proc. NIPS\\nDeep Learning Workshop . [13] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep convolutional\\nneural networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012) . [14] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations in\\nvector space. In International Conference on Learning Representations: Workshops Track . [15] Russakovsky, O. and Fei-Fei, L. (2010). Attribute learning in large-scale datasets. In European Confer-\\nence of Computer Vision (ECCV), International Workshop on Parts and Attributes , Crete, Greece. [16] Srivastava, N. and Salakhutdinov, R.', metadata={'summary': 'This set of references covers various aspects of machine learning, including image retrieval, object recognition, multimodal neural language models, deep convolutional neural networks, word representation estimation, attribute learning, and deep learning architectures.'}),\n",
              " Document(page_content='(2012). Multimodal learning with deep boltzmann machines. In\\nNIPS’2012 . [17] Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., and Rabi-\\nnovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842 . 7', metadata={'summary': 'In 2012, a study introduced Multimodal Learning with Deep Boltzmann Machines, while in 2014, another research team proposed \"Going Deeper with Convolutions\" using deep learning techniques.'})]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4."
      ],
      "metadata": {
        "id": "MkgQr5301cgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_text = [i.page_content for i in docs]\n",
        "docs_summary = [i.metadata[\"summary\"] for i in docs]\n",
        "# docs_text"
      ],
      "metadata": {
        "id": "HuJCul0LG8Pt"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"/content/database_chromadb\""
      ],
      "metadata": {
        "id": "BnrEix0yyd18"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.vectorstores import Chroma\n",
        "# vector_store = Chroma.from_texts(docs_text,\n",
        "#                                  embeddings,\n",
        "#                                  metadatas = [{\"source\" : s} for s in docs_summary],\n",
        "#                                  persist_directory = persist_directory)"
      ],
      "metadata": {
        "id": "u55nbG2cyd0m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "vector_store = Chroma.from_texts(docs_text,\n",
        "                                 embeddings,\n",
        "                                 metadatas = [{\"summary\" : s} for s in docs_summary],\n",
        "                                 persist_directory = persist_directory)"
      ],
      "metadata": {
        "id": "nVvMuRPiJQ7R"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH6_84at-PfB",
        "outputId": "6a8f9b6f-4db6-4dc6-96eb-ff430f254f2b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7bf4e07175b0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.persist()\n",
        "vector_store = None"
      ],
      "metadata": {
        "id": "8Tn8-nnJydzK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = Chroma(persist_directory = persist_directory,\n",
        "                      embedding_function = embeddings)"
      ],
      "metadata": {
        "id": "-mNYL3YeydxS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs = {\"k\" : 10})"
      ],
      "metadata": {
        "id": "l40PFq-3ydvf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU8RzwMwydt8",
        "outputId": "49819d8d-2fa6-444f-fd79-182a1f7ca7d3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7bf4e075bcd0>, search_kwargs={'k': 10})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_docs = retriever.get_relevant_documents(\"tell me about the discriminator loss function\")\n",
        "retriever_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gZrRfnRydsL",
        "outputId": "d56e2316-873b-4100-b9e5-d1291b8e34cd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='(2012). Multimodal learning with deep boltzmann machines. In\\nNIPS’2012 . [17] Szegedy, C., Liu, W., Jia, Y ., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V ., and Rabi-\\nnovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842 . 7', metadata={'summary': 'In 2012, a study introduced Multimodal Learning with Deep Boltzmann Machines, while in 2014, another research team proposed \"Going Deeper with Convolutions\" using deep learning techniques.'}),\n",
              " Document(page_content='2Yahoo Flickr Creative Common 100M http://webscope.sandbox.yahoo.com/catalog.', metadata={'summary': 'Yahoo Flickr Creative Common 100M is a dataset of 100 million Creative Common licensed images from Flickr. It is hosted by Yahoo Webscope and can be accessed through its catalog.'}),\n",
              " Document(page_content='Dropout\\nwith probability of 0.5 was applied to both the generator and discriminator. The hyper-parameters and architectural choices were obtained by cross-validation and a mix of\\nrandom grid search and manual selection (albeit over a somewhat limited search space.)\\n5 Future Work\\nThe results shown in this paper are extremely preliminary, but they demonstrate the potential of\\nconditional adversarial nets and show promise for interesting and useful applications. In future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.In future explorations between now and the workshop we expect to present more sophisticated mod-\\nels, as well as a more detailed and thorough analysis of their performance and characteristics.', metadata={'summary': 'Dropout with a probability of 0.5 was applied to both the generator and discriminator networks. Hyperparameters and architectural choices were obtained through cross-validation and a combination of random grid search and manual selection.'}),\n",
              " Document(page_content='The discriminator maps xto a maxout [6] layer with 240 units and 5 pieces, and yto a maxout layer\\nwith 50 units and 5 pieces. Both of the hidden layers mapped to a joint maxout layer with 240 units\\nand 4 pieces before being fed to the sigmoid layer. (The precise architecture of the discriminator\\nis not critical as long as it has sufﬁcient power; we have found that maxout units are typically well\\nsuited to the task.)\\nThe model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7. Dropout\\n[9] with probability of 0.5 was applied to both the generator and discriminator. And best estimate of\\nlog-likelihood on the validation set was used as stopping point. Table 1 shows Gaussian Parzen window log-likelihood estimate for the MNIST dataset test data. 1000 samples were drawn from each 10 class and a Gaussian Parzen window was ﬁtted to these\\nsamples. We then estimate the log-likelihood of the test set using the Parzen window distribution. (See [8] for more details of how this estimate is constructed.)\\nThe conditional adversarial net results that we present are comparable with some other network\\nbased, but are outperformed by several other approaches – including non-conditional adversarial\\nnets. We present these results more as a proof-of-concept than as demonstration of efﬁcacy, and\\nbelieve that with further exploration of hyper-parameter space and architecture that the conditional\\nmodel should match or exceed the non-conditional results. Fig 2 shows some of the generated samples.', metadata={'summary': 'The discriminator network in a conditional adversarial network consists of multiple maxout layers with varying units and pieces, followed by a joint maxout layer and a sigmoid layer. The model is trained using stochastic gradient descent with various optimization techniques such as momentum, learning rate decay, and dropout. The Gaussian Parzen window log-likelihood estimate is used as a stopping point for training. The conditional adversarial net results are comparable to other network-based methods but have potential for further improvement with hyperparameter optimization and architectural exploration.'}),\n",
              " Document(page_content='(2008). The mir ﬂickr retrieval evaluation. In MIR ’08: Proceedings of the\\n2008 ACM International Conference on Multimedia Information Retrieval , New York, NY , USA. ACM. [11] Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y . (2009). What is the best multi-stage architecture\\nfor object recognition? In ICCV’09 . [12] Kiros, R., Zemel, R., and Salakhutdinov, R. (2013). Multimodal neural language models. In Proc. NIPS\\nDeep Learning Workshop . [13] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet classiﬁcation with deep convolutional\\nneural networks. In Advances in Neural Information Processing Systems 25 (NIPS’2012) . [14] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efﬁcient estimation of word representations in\\nvector space. In International Conference on Learning Representations: Workshops Track . [15] Russakovsky, O. and Fei-Fei, L. (2010). Attribute learning in large-scale datasets. In European Confer-\\nence of Computer Vision (ECCV), International Workshop on Parts and Attributes , Crete, Greece. [16] Srivastava, N. and Salakhutdinov, R.', metadata={'summary': 'This set of references covers various aspects of machine learning, including image retrieval, object recognition, multimodal neural language models, deep convolutional neural networks, word representation estimation, attribute learning, and deep learning architectures.'}),\n",
              " Document(page_content='Another obvious direction left for future work is to construct a joint training scheme to learn the\\nlanguage model. Works such as [12] has shown that we can learn a language model for suited for\\nthe speciﬁc task. Acknowledgments\\nThis project was developed in Pylearn2 [7] framework, and we would like to thank Pylearn2 devel-\\nopers. We also like to thank Ian Goodfellow for helpful discussion during his afﬁliation at University\\nof Montreal. The authors gratefully acknowledge the support from the Vision & Machine Learning,\\nand Production Engineering teams at Flickr (in alphabetical order: Andrew Stadlen, Arel Cordero,\\nClayton Mellina, Cyprien Noel, Frank Liu, Gerry Pesavento, Huy Nguyen, Jack Culpepper, John\\nKo, Pierre Garrigues, Rob Hess, Stacey Svetlichnaya, Tobi Baumgartner, and Ye Lu). References\\n[1] Bengio, Y ., Mesnil, G., Dauphin, Y ., and Rifai, S. (2013). Better mixing via deep representations. In\\nICML’2013 . [2] Bengio, Y ., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014). Deep generative stochastic net-\\nworks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning\\n(ICML’14) . 6[3] Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Mikolov, T., et al. (2013). Devise: A deep\\nvisual-semantic embedding model. In Advances in Neural Information Processing Systems , pages 2121–\\n2129. [4] Glorot, X., Bordes, A., and Bengio, Y . (2011). Deep sparse rectiﬁer neural networks. In International\\nConference on Artiﬁcial Intelligence and Statistics , pages 315–323. [5] Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y . (2013a). Multi-prediction deep boltzmann ma-\\nchines. In Advances in Neural Information Processing Systems , pages 548–556. [6] Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013b). Maxout networks. InICML’2013 . [7] Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V ., Mirza, M., Pascanu, R., Bergstra, J.,\\nBastien, F., and Bengio, Y . (2013c). Pylearn2: a machine learning research library. arXiv preprint\\narXiv:1308.4214 . [8] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\\nBengio, Y . (2014). Generative adversarial nets. In NIPS’2014 . [9] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2012). Improving\\nneural networks by preventing co-adaptation of feature detectors. Technical report, arXiv:1207.0580. [10] Huiskes, M. J. and Lew, M. S.', metadata={'summary': 'A potential area for further research is developing a joint training method for a language model. Previous studies have demonstrated the feasibility of creating task-specific language models.'}),\n",
              " Document(page_content='We use the output of the last fully connected layer with\\n4096 units as image representations. For the world representation we ﬁrst gather a corpus of text from concatenation of user-tags, titles\\nand descriptions from YFCC100M2dataset metadata. After pre-processing and cleaning of the\\ntext we trained a skip-gram model [14] with word vector size of 200. And we omitted any word\\nappearing less than 200 times from the vocabulary, thereby ending up with a dictionary of size\\n247465. We keep the convolutional model and the language model ﬁxed during training of the adversarial\\nnet. And leave the experiments when we even backpropagate through these models as future work. For our experiments we use MIR Flickr 25,000 dataset [10], and extract the image and tags features\\nusing the convolutional model and language model we described above. Images without any tag\\nwere omitted from our experiments and annotations were treated as extra tags. The ﬁrst 150,000\\nexamples were used as training set. Images with multiple tags were repeated inside the training set\\nonce for each associated tag. For evaluation, we generate 100 samples for each image and ﬁnd top 20 closest words using cosine\\nsimilarity of vector representation of the words in the vocabulary to each sample. Then we select\\nthe top 10 most common words among all 100 samples. Table 4.2 shows some samples of the user\\nassigned tags and annotations along with the generated tags. The best working model’s generator receives Gaussian noise of size 100 as noise prior and maps it\\nto 500 dimension ReLu layer. And maps 4096 dimension image feature vector to 2000 dimension\\nReLu hidden layer. Both of these layers are mapped to a joint representation of 200 dimension linear\\nlayer which would output the generated word vectors. The discriminator is consisted of 500 and 1200 dimension ReLu hidden layers for word vectors and\\nimage features respectively and maxout layer with 1000 units and 3 pieces as the join layer which is\\nﬁnally fed to the one single sigmoid unit. The model was trained using stochastic gradient decent with mini-batches of size 100 and ini-\\ntial learning rate of 0.1which was exponentially decreased down to .000001 with decay factor of\\n1.00004 . Also momentum was used with initial value of .5which was increased up to 0.7.', metadata={'summary': 'This work uses a skip-gram model to represent text from image metadata as word vectors. Image representations are obtained from a convolutional model. An adversarial network is trained to map Gaussian noise and image features to word vectors. The model generates tags that are similar to user-assigned tags.'}),\n",
              " Document(page_content='php?datatype=i&did=67 . 5User tags + annotations Generated tags\\nmontanha, trem, inverno,\\nfrio, people, male, plant\\nlife, tree, structures, trans-\\nport, cartaxi, passenger, line,\\ntransportation, railway\\nstation, passengers,\\nrailways, signals, rail,\\nrails\\nfood, raspberry, delicious,\\nhomemadechicken, fattening,\\ncooked, peanut, cream,\\ncookie, house made,\\nbread, biscuit, bakes\\nwater, rivercreek, lake, along, near,\\nriver, rocky, treeline, val-\\nley, woods, waters\\npeople, portrait, female,\\nbaby, indoorlove, people, posing, girl,\\nyoung, strangers, pretty,\\nwomen, happy, life\\nTable 2: Samples of generated tags\\nAlso, in the current experiments we only use each tag individually. But by using multiple tags at\\nthe same time (effectively posing generative problem as one of ‘set generation’) we hope to achieve\\nbetter results.', metadata={'summary': 'This phrase describes a user-generated tagging system for images, where users can add tags to images to describe their content. The system then uses these tags to generate additional tags that are relevant to the image. The phrase also mentions that the system is currently only using each tag individually, but that the researchers hope to achieve better results by using multiple tags at the same time.'}),\n",
              " Document(page_content='Each row is conditioned on one label and each column\\nis a different generated sample. Figure 2: Generated MNIST digits, each row conditioned on one label\\n4.2 Multimodal\\nPhoto sites such as Flickr are a rich source of labeled data in the form of images and their associated\\nuser-generated metadata (UGM) — in particular user-tags. 4User-generated metadata differ from more ‘canonical’ image labelling schems in that they are typ-\\nically more descriptive, and are semantically much closer to how humans describe images with\\nnatural language rather than just identifying the objects present in an image. Another aspect of\\nUGM is that synoymy is prevalent and different users may use different vocabulary to describe the\\nsame concepts — consequently, having an efﬁcient way to normalize these labels becomes impor-\\ntant. Conceptual word embeddings [14] can be very useful here since related concepts end up being\\nrepresented by similar vectors. In this section we demonstrate automated tagging of images, with multi-label predictions, using con-\\nditional adversarial nets to generate a (possibly multi-modal) distribution of tag-vectors conditional\\non image features. For image features we pre-train a convolutional model similar to the one from [13] on the full\\nImageNet dataset with 21,000 labels [15].', metadata={'summary': 'In conditional adversarial networks, each row represents a label, and each column is a generated sample. The network generates MNIST digits, with each row conditioned on a specific label. The model also handles multimodal data, such as user-generated metadata for images, where multiple labels may describe the same concept.'}),\n",
              " Document(page_content='\\n\\nConditional Generative Adversarial Nets\\nMehdi Mirza\\nD´epartement d’informatique et de recherche op ´erationnelle\\nUniversit ´e de Montr ´eal\\nMontr ´eal, QC H3C 3J7\\nmirzamom@iro.umontreal.ca\\nSimon Osindero\\nFlickr / Yahoo Inc. San Francisco, CA 94103\\nosindero@yahoo-inc.com\\nAbstract\\nGenerative Adversarial Nets [8] were recently introduced as a novel way to train\\ngenerative models. In this work we introduce the conditional version of generative\\nadversarial nets, which can be constructed by simply feeding the data, y, we wish\\nto condition on to both the generator and discriminator. We show that this model\\ncan generate MNIST digits conditioned on class labels. We also illustrate how\\nthis model could be used to learn a multi-modal model, and provide preliminary\\nexamples of an application to image tagging in which we demonstrate how this\\napproach can generate descriptive tags which are not part of training labels. 1 Introduction\\nGenerative adversarial nets were recently introduced as an alternative framework for training gen-\\nerative models in order to sidestep the difﬁculty of approximating many intractable probabilistic\\ncomputations. Adversarial nets have the advantages that Markov chains are never needed, only backpropagation is\\nused to obtain gradients, no inference is required during learning, and a wide variety of factors and\\ninteractions can easily be incorporated into the model. Furthermore, as demonstrated in [8], it can produce state of the art log-likelihood estimates and\\nrealistic samples. In an unconditioned generative model, there is no control on modes of the data being generated. However, by conditioning the model on additional information it is possible to direct the data gener-\\nation process. Such conditioning could be based on class labels, on some part of data for inpainting\\nlike [5], or even on data from different modality. In this work we show how can we construct the conditional adversarial net. And for empirical results\\nwe demonstrate two set of experiment. One on MNIST digit data set conditioned on class labels and\\none on MIR Flickr 25,000 dataset [10] for multi-modal learning. 1arXiv:1411.1784v1  [cs.LG]  6 Nov 20142 Related Work\\n2.1 Multi-modal Learning For Image Labelling\\nDespite the many recent successes of supervised neural networks (and convolutional networks in\\nparticular) [13, 17], it remains challenging to scale such models to accommodate an extremely large\\nnumber of predicted output categories. A second issue is that much of the work to date has focused\\non learning one-to-one mappings from input to output. However, many interesting problems are\\nmore naturally thought of as a probabilistic one-to-many mapping. For instance in the case of\\nimage labeling there may be many different tags that could appropriately applied to a given image,\\nand different (human) annotators may use different (but typically synonymous or related) terms to\\ndescribe the same image. One way to help address the ﬁrst issue is to leverage additional information from other modalities:\\nfor instance, by using natural language corpora to learn a vector representation for labels in which\\ngeometric relations are semantically meaningful. When making predictions in such spaces, we ben-\\neﬁt from the fact that when prediction errors we are still often ‘close’ to the truth (e.g. predicting\\n’table’ instead of ’chair’), and also from the fact that we can naturally make predictive generaliza-\\ntions to labels that were not seen during training time. Works such as [3] have shown that even a\\nsimple linear mapping from image feature-space to word-representation-space can yield improved\\nclassiﬁcation performance. One way to address the second problem is to use a conditional probabilistic generative model, the\\ninput is taken to be the conditioning variable and the one-to-many mapping is instantiated as a\\nconditional predictive distribution. [16] take a similar approach to this problem, and train a multi-modal Deep Boltzmann Machine on\\nthe MIR Flickr 25,000 dataset as we do in this work. Additionally, in [12] the authors show how to train a supervised multi-modal neural language model,\\nand they are able to generate descriptive sentence for images. 3 Conditional Adversarial Nets\\n3.1 Generative Adversarial Nets\\nGenerative adversarial nets were recently introduced as a novel way to train a generative model. They consists of two ‘adversarial’ models: a generative model Gthat captures the data distribution,\\nand a discriminative model Dthat estimates the probability that a sample came from the training\\ndata rather than G. BothGandDcould be a non-linear mapping function, such as a multi-layer\\nperceptron. To learn a generator distribution pgover data data x, the generator builds a mapping function from\\na prior noise distribution pz(z)to data space as G(z;θg). And the discriminator, D(x;θd), outputs\\na single scalar representing the probability that xcame form training data rather than pg. GandDare both trained simultaneously: we adjust parameters for Gto minimize log(1−D(G(z))\\nand adjust parameters for Dto minimize logD (X), as if they are following the two-player min-max\\ngame with value function V(G,D ):\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))]. (1)\\n3.2 Conditional Adversarial Nets\\nGenerative adversarial nets can be extended to a conditional model if both the generator and discrim-\\ninator are conditioned on some extra information y.ycould be any kind of auxiliary information,\\nsuch as class labels or data from other modalities. We can perform the conditioning by feeding y\\ninto the both the discriminator and generator as additional input layer. 2In the generator the prior input noise pz(z), andyare combined in joint hidden representation, and\\nthe adversarial training framework allows for considerable ﬂexibility in how this hidden representa-\\ntion is composed.1\\nIn the discriminator xandyare presented as inputs and to a discriminative function (embodied\\nagain by a MLP in this case). The objective function of a two-player minimax game would be as Eq 2\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x|y)] +Ez∼pz(z)[log(1−D(G(z|y)))]. (2)\\nFig 1 illustrates the structure of a simple conditional adversarial net. Figure 1: Conditional adversarial net\\n4 Experimental Results\\n4.1 Unimodal\\nWe trained a conditional adversarial net on MNIST images conditioned on their class labels, encoded\\nas one-hot vectors. In the generator net, a noise prior zwith dimensionality 100 was drawn from a uniform distribution\\nwithin the unit hypercube. Both zandyare mapped to hidden layers with Rectiﬁed Linear Unit\\n(ReLu) activation [4, 11], with layer sizes 200 and 1000 respectively, before both being mapped to\\nsecond, combined hidden ReLu layer of dimensionality 1200. We then have a ﬁnal sigmoid unit\\nlayer as our output for generating the 784-dimensional MNIST samples. 1For now we simply have the conditioning input and prior noise as inputs to a single hidden layer of a MLP,\\nbut one could imagine using higher order interactions allowing for complex generation mechanisms that would\\nbe extremely difﬁcult to work with in a traditional generative framework. 3Model MNIST\\nDBN [1] 138±2\\nStacked CAE [1] 121±1.6\\nDeep GSN [2] 214±1.1\\nAdversarial nets 225±2\\nConditional adversarial nets 132±1.8\\nTable 1: Parzen window-based log-likelihood estimates for MNIST. We followed the same procedure as [8]\\nfor computing these values.', metadata={'summary': 'Conditional Generative Adversarial Nets (Conditional GANs) extend Generative Adversarial Nets by conditioning both the generator and discriminator on additional information (e.g., class labels, data from other modalities). This enables directed data generation based on the conditioning information, opening up applications in multi-modal learning, image tagging, and other domains where control over the generated data is desired. Conditional GANs have shown promising results in generating data conditioned on class labels and learning multi-modal models, demonstrating the flexibility and potential of adversarial networks in conditional generation tasks.'})]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# create the chain to answer questions\n",
        "llm1 = ChatGoogleGenerativeAI(model = \"gemini-pro\",\n",
        "                             google_api_key = GOOGLE_API_KEY,\n",
        "                             convert_system_message_to_human = True)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm = llm1,\n",
        "                                       chain_type = \"stuff\",\n",
        "                                       retriever = retriever,\n",
        "                                       )"
      ],
      "metadata": {
        "id": "zY2PMIf8ydqY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])"
      ],
      "metadata": {
        "id": "RRa8RQJ7ydob"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "querys =  \"Provide me the objective function of the GAN model.\" # [\"tell me about the discriminator loss\", \"Share me the generator loss\", \"Provide me the objective function of the GAN model.\"]\n",
        "llm_response = qa_chain.invoke(querys)\n",
        "llm_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cND1Pe0H0rZv",
        "outputId": "90da771c-fbf5-4044-f706-01fd3bb5f6a2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Provide me the objective function of the GAN model.',\n",
              " 'result': 'The objective function of the GAN model is:\\nmin\\nGmax\\nDV(D,G ) =Ex∼pdata(x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))].'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHuKlj5s0rWR",
        "outputId": "de829889-5de6-4bbf-acf9-cadc58a8d94c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The objective function of the GAN model is:\n",
            "min\n",
            "Gmax\n",
            "DV(D,G ) =Ex∼pdata(x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "oo9_lTGH0rUl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(llm_response['query'])"
      ],
      "metadata": {
        "id": "0Ms3zPLz0rS7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "f18da5e6-cc87-4fc8-ca09-4f26cd0ac5ff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Provide me the objective function of the GAN model."
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(llm_response['result'])"
      ],
      "metadata": {
        "id": "B2h3S1Fr0rRq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "2f1ec6c4-15fa-4bb6-e171-0f1b09f63a36"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The objective function of the GAN model is:\nmin\nGmax\nDV(D,G ) =Ex∼pdata(x)[logD(x)] +Ez∼pz(z)[log(1−D(G(z)))]."
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuAVfQ9j0rQJ"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}
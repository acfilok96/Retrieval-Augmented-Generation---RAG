{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBu0uf-Tp_fL"
      },
      "outputs": [],
      "source": [
        "# !pip install -q langchain langchain_community openai tiktoken pinecone-client langchain-pinecone PyPDF2 pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community openai tiktoken langchain-pinecone PyPDF2 pypdf langchain langchain-openai pinecone-client langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mub9IOKltOeY",
        "outputId": "0816959b-7975-4531-9c6d-b04754e95b62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.8/811.8 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_TkWIA-6p_fP"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import os, PyPDF2, pypdf\n",
        "import langchain\n",
        "import langchain_community\n",
        "import openai\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
        "import pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone as pn\n",
        "from langchain.llms import OpenAI\n",
        "from IPython.display import display, Markdown, Latex, HTML, JSON\n",
        "# from langchain.vectorstores.pinecone import PineconeStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zjAfcc0Kp_fW"
      },
      "outputs": [],
      "source": [
        "# Load Document\n",
        "\n",
        "def read_doc_file(directory):\n",
        "    file_loader = PyPDFDirectoryLoader(directory)\n",
        "    documents = file_loader.load()\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ueS_OqBp_fY",
        "outputId": "9cc609c6-c858-4996-fce7-318b1222507d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "doc = read_doc_file(\"/content/file\")\n",
        "\n",
        "len(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8KZZKUoJp_fZ"
      },
      "outputs": [],
      "source": [
        "# Convert doc into chunks\n",
        "# Divide the docs into chunks\n",
        "\n",
        "def chunk_data(docs, chunk_size = 800, chunk_overlap = 50):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size,\n",
        "                                                   chunk_overlap = chunk_overlap)\n",
        "    docs = text_splitter.split_documents(docs)\n",
        "    return docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gt7jpcFp_fa",
        "outputId": "5362db03-87a9-42d5-aa61-6b9dc0980be4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "documents = chunk_data(doc)\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfch66qap_fb",
        "outputId": "ddaec130-6ab4-47bd-8b83-e9a6d0877fbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7c495d10b130>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7c49313eee30>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Embedding technique\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key = \" \")\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FkxByMEEp_fb"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "\n",
        "# vectors = embeddings.embed_query(documents)\n",
        "# len(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_4MR7Y0p_fc"
      },
      "outputs": [],
      "source": [
        "# vector Search Database in Pinecone\n",
        "\n",
        "# pinecone.init(\n",
        "#     api_key = \" \",\n",
        "#     environment = \"gcp-starter\"\n",
        "# )\n",
        "# index_name = \"rag-one\"\n",
        "# pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q pinecone-client"
      ],
      "metadata": {
        "id": "lLulPap0m3gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# pc = Pinecone(api_key=' ')\n",
        "\n",
        "# pc.create_index(name = \"rag-one\",\n",
        "#                 dimension = 1536,\n",
        "#                 metric = \"cosine\",\n",
        "#                 spec = ServerlessSpec(cloud = 'gcp-starter',\n",
        "#                                     region = 'Iowa (us-central1)'\n",
        "#                                     )\n",
        "#                 )"
      ],
      "metadata": {
        "id": "hgmno_UIlrza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pinecone import Pinecone\n",
        "\n",
        "# pinecone_pn = Pinecone(api_key=\" \",\n",
        "#                        environment = \"gcp-starter\")\n",
        "# index_name = \"rag-one\"\n",
        "# pinecone_database = pinecone_pn.Index(index_name)"
      ],
      "metadata": {
        "id": "Vjyj93F4uIen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# pc = Pinecone(api_key='YOUR_API_KEY')\n",
        "\n",
        "# pc.create_index(\n",
        "#     name=\"quickstart\",\n",
        "#     dimension=1536,\n",
        "#     metric=\"cosine\",\n",
        "#     spec=ServerlessSpec(\n",
        "#         cloud='gcp-starter',\n",
        "#         region='Iowa (us-central1)'\n",
        "#     )\n",
        "# )"
      ],
      "metadata": {
        "id": "DK6oqx4zuN0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q langchain_pinecone"
      ],
      "metadata": {
        "id": "fIBf8bVgnwLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pinecone-client\n",
        "\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = \" \"\n",
        "index_name = \"rag-one\"\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key = api_key, index_name = index_name)"
      ],
      "metadata": {
        "id": "1Aeq63nuw8AM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhz9PZf64RDE",
        "outputId": "493c5f40-81dd-4653-c6e8-111207a1d6a9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'indexes': [{'dimension': 1536,\n",
              "              'host': 'rag-one-gdpu1ee.svc.gcp-starter.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'rag-one',\n",
              "              'spec': {'pod': {'environment': 'gcp-starter',\n",
              "                               'pod_type': 'starter',\n",
              "                               'pods': 1,\n",
              "                               'replicas': 1,\n",
              "                               'shards': 1}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WauW0rHaxHXG",
        "outputId": "82293edb-de45-48a7-c3f5-2e5ba22cf93d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = api_key\n",
        "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
        "PINECONE_ENV = \"gcp-starter\""
      ],
      "metadata": {
        "id": "9rOVQ4jI_gQs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "# from pinecone.index import Index\n",
        "\n",
        "api_key = \" \"\n",
        "index_name = \"rag-one\"\n",
        "\n",
        "# index = Index(index_name)\n",
        "\n",
        "docsearch = Pinecone.from_documents(doc, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "Nj6JuOZ19ckj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_pinecone import Pinecone\n",
        "# # from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "# vectorstore = Pinecone(index,\n",
        "#                        embeddings.embed_query,\n",
        "#                        index_name\n",
        "#                       )\n",
        "\n",
        "\n",
        "# docsearch = vectorstore.from_documents(doc, embeddings, api_key = api_key) # , index_name=index_name)"
      ],
      "metadata": {
        "id": "fqEt7ynFmLgX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.vectorstores import Pinecone\n",
        "\n",
        "# docsearch = pc.from_documents(doc, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "IpVtCmF33jih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "# docs = vectorstore.similarity_search(query)\n",
        "# print(docs)"
      ],
      "metadata": {
        "id": "zc1sQPIymTQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DaTOQuj6mWM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO29zgDEp_fe"
      },
      "outputs": [],
      "source": [
        "# # cosine similarity to retrive results from vector database\n",
        "\n",
        "# def retrive_query(query, vectorstore,  k = 2):\n",
        "#     matching_result = vectorstore.similarity_search(query, k = k)\n",
        "#     return matching_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "v1SKLt_ip_ff"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "# from langchain import OpenAI\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "OPENAI_API_KEY = \" \"\n",
        "\n",
        "# completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=docsearch.as_retriever()\n",
        ")\n"
      ],
      "metadata": {
        "id": "kXuBGhBh0pdR"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"tell me about the Abstract of Generative Adversarial Nets.\"\n",
        "response = qa.invoke(user_query)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hydLPkyC08ec",
        "outputId": "5f481e3d-5c65-4dd6-9bf2-d1ec0009075b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'tell me about the Abstract of Generative Adversarial Nets.',\n",
              " 'result': 'The abstract of the paper \"Generative Adversarial Nets\" introduces a new framework for estimating generative models using an adversarial process. The framework involves training two models simultaneously: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. The paper demonstrates the potential of this framework through qualitative and quantitative evaluation of the generated samples.'}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "udd381XSAhJ-",
        "outputId": "73fbed46-d0a8-4a27-adfd-7ba385110d09"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The abstract of the paper \"Generative Adversarial Nets\" introduces a new framework for estimating generative models using an adversarial process. The framework involves training two models simultaneously: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. The paper demonstrates the potential of this framework through qualitative and quantitative evaluation of the generated samples."
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCY9rasop_fg"
      },
      "outputs": [],
      "source": [
        "# llm = OpenAI(model_name = \"text-davinci-003\", openai_api_key = \" \", temperature = 0.0)\n",
        "# chain = load_qa_chain(llm, chain_type = \"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ObBeeCbp_fh"
      },
      "outputs": [],
      "source": [
        "# # search answers from vector database\n",
        "\n",
        "# def retrive_answers(query, vectorstore, chain):\n",
        "#     doc_search = retrive_query(query, vectorstore)\n",
        "#     response = chain.run(input_documents = doc_search,\n",
        "#                          question = query)\n",
        "#     return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNWA8P_ep_fj"
      },
      "outputs": [],
      "source": [
        "# user_query = \"Summarize the GAN paper.\"\n",
        "# answer = retrive_answers(user_query, vectorstore, chain)\n",
        "# # Markdown(answer[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSTh4iLnp_fq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-D9wnGnp_fr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "968AVoeqp_fs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CCjwC_Np_fs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmFpkpU4p_fs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

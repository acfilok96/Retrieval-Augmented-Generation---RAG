{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mBu0uf-Tp_fL"
      },
      "outputs": [],
      "source": [
        "# !pip install -q langchain langchain_community openai tiktoken pinecone-client langchain-pinecone PyPDF2 pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community openai tiktoken langchain-pinecone PyPDF2 pypdf langchain langchain-openai pinecone-client langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mub9IOKltOeY",
        "outputId": "2310852b-ed01-4c1a-d29c-ef9529577c52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.8/811.8 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_TkWIA-6p_fP"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "import os, PyPDF2, pypdf\n",
        "import langchain\n",
        "import langchain_community\n",
        "import openai\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader, PyPDFLoader\n",
        "import pinecone\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone as pn\n",
        "from langchain.llms import OpenAI\n",
        "from IPython.display import display, Markdown, Latex, HTML, JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zjAfcc0Kp_fW"
      },
      "outputs": [],
      "source": [
        "# Load Document\n",
        "\n",
        "def read_doc_file(directory):\n",
        "    file_loader = PyPDFDirectoryLoader(directory)\n",
        "    documents = file_loader.load()\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ueS_OqBp_fY",
        "outputId": "9fcf2e35-c122-47e5-bf67-07fb79c5f78f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "doc = read_doc_file(\"/content/file\")\n",
        "\n",
        "len(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8KZZKUoJp_fZ"
      },
      "outputs": [],
      "source": [
        "# Convert doc into chunks\n",
        "# Divide the docs into chunks\n",
        "\n",
        "def chunk_data(docs, chunk_size = 800, chunk_overlap = 50):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size,\n",
        "                                                   chunk_overlap = chunk_overlap)\n",
        "    docs = text_splitter.split_documents(docs)\n",
        "    return docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gt7jpcFp_fa",
        "outputId": "b027041e-9b4a-4bfa-97d8-ecb9caec7abb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "documents = chunk_data(doc)\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfch66qap_fb",
        "outputId": "c9e34c37-1f46-42c5-8f7b-62eddfd945a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7ea1dab93340>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7ea1da916140>, model='text-embedding-ada-002', dimensions=None, deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Embedding technique\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key = \" \")\n",
        "embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pinecone-client\n",
        "\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = \" \"\n",
        "index_name = \"rag-one\"\n",
        "\n",
        "# configure client\n",
        "pc = Pinecone(api_key = api_key, index_name = index_name)"
      ],
      "metadata": {
        "id": "1Aeq63nuw8AM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.list_indexes()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhz9PZf64RDE",
        "outputId": "05464f65-c3f0-4f55-b73a-6110948c6eb7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'indexes': [{'dimension': 1536,\n",
              "              'host': 'rag-one-gdpu1ee.svc.gcp-starter.pinecone.io',\n",
              "              'metric': 'cosine',\n",
              "              'name': 'rag-one',\n",
              "              'spec': {'pod': {'environment': 'gcp-starter',\n",
              "                               'pod_type': 'starter',\n",
              "                               'pods': 1,\n",
              "                               'replicas': 1,\n",
              "                               'shards': 1}},\n",
              "              'status': {'ready': True, 'state': 'Ready'}}]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WauW0rHaxHXG",
        "outputId": "ffe3cfbd-7ccc-40e5-9f07-1e70d23c6492"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 9e-05,\n",
              " 'namespaces': {'': {'vector_count': 9}},\n",
              " 'total_vector_count': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = api_key\n",
        "PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
        "PINECONE_ENV = \"gcp-starter\""
      ],
      "metadata": {
        "id": "9rOVQ4jI_gQs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "api_key = \" \"\n",
        "index_name = \"rag-one\"\n",
        "\n",
        "docsearch = Pinecone.from_documents(doc, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "Nj6JuOZ19ckj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "OPENAI_API_KEY = \" \"\n",
        "\n",
        "# completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=docsearch.as_retriever()\n",
        ")\n"
      ],
      "metadata": {
        "id": "kXuBGhBh0pdR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"tell me about the Abstract of Generative Adversarial Nets and share me the writer of the document in another paragraph\"\n",
        "response = qa.invoke(user_query)\n",
        "Markdown(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "hydLPkyC08ec",
        "outputId": "84148ec6-3e7e-43c8-d740-7af14d5daeb5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The abstract of the document \"Generative Adversarial Nets\" describes a new framework for estimating generative models using an adversarial process. The framework involves training two models simultaneously: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. The authors demonstrate the potential of this framework through qualitative and quantitative evaluation of the generated samples.\n\nThe document is written by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio."
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmFpkpU4p_fs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Original Link:*** https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-subdoc-summary/examples/subdoc-summary.ipynb"
      ],
      "metadata": {
        "id": "k2juu8mad8j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sub-Document Summary Metadata Pack**\n",
        "\n",
        "This LlamaPack provides an advanced technique for injecting each chunk with \"sub-document\" metadata. This context augmentation technique is helpful for both retrieving relevant context and for synthesizing correct answers.\n",
        "\n",
        "It is a step beyond simply adding a summary of the document as the metadata to each chunk. Within a long document, there can be multiple distinct themes, and we want each chunk to be grounded in global but relevant context."
      ],
      "metadata": {
        "id": "MyJ-9CSsdN3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q llama-index-packs-subdoc-summary llama-index-llms-openai llama-index-embeddings-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPFNyWbdZdBz",
        "outputId": "8b5bd8fe-dab4-4d62-e809-8f7cb3ae2c7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index openai"
      ],
      "metadata": {
        "id": "8WwHlHUDZimN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load & Setup Data**"
      ],
      "metadata": {
        "id": "pmreMBO4dczJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS671OIqZKQF",
        "outputId": "762cda00-cf79-4ed9-f177-968eea69d97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  6  518k    6 32819    0     0   172k      0  0:00:03 --:--:--  0:00:03  172k\r100  518k  100  518k    0     0  2273k      0 --:--:-- --:--:-- --:--:-- 2272k\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/'\n",
        "!curl 'https://arxiv.org/pdf/1406.2661.pdf' -o 'data/Generative Adversarial Nets.pdf'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()"
      ],
      "metadata": {
        "id": "HsQ6ny_IZcN6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run the Sub-Document Summary Metadata Pack**"
      ],
      "metadata": {
        "id": "EtrGoLMEdmto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.packs.subdoc_summary import SubDocSummaryPack\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "import openai\n",
        "\n",
        "OPENAI_API_KEY = \"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\"\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "subdoc_summary_pack = SubDocSummaryPack(\n",
        "    documents,\n",
        "    parent_chunk_size = 8192,  # default,\n",
        "    child_chunk_size = 512,  # default\n",
        "    llm = OpenAI(model = \"gpt-3.5-turbo\"),\n",
        "    embed_model = OpenAIEmbedding(),\n",
        ")"
      ],
      "metadata": {
        "id": "U6cPlvDlZcMQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "from llama_index.core.response.notebook_utils import display_source_node"
      ],
      "metadata": {
        "id": "DQTlnETvc6Br"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example - 1**"
      ],
      "metadata": {
        "id": "VRW9sjj5dqNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = subdoc_summary_pack.run(\"Summarize the GAN paper.\")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "0IcmI8u2ZcK7",
        "outputId": "6fea9c66-6216-4ec0-d749-c36a13e3a71d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper discusses Generative Adversarial Networks (GANs), which involve a two-player minimax game between a generator and a discriminator. The training process alternates between optimizing the discriminator and the generator iteratively to prevent overfitting and ensure effective learning. GANs aim to recover the data generating distribution without the need for Markov chains, relying on backpropagation for training gradients. The framework of GANs allows for straightforward extensions such as conditional generative models and learned approximate inference. The advantages of GANs include computational efficiency, statistical advantages from indirect updates through gradients, and the ability to represent sharp distributions compared to methods relying on Markov chains."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    display_source_node(n, source_length=10000, metadata_mode=\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "Ir4u6ezFdDB8",
        "outputId": "793e70e5-b68f-4f1f-f2ae-ff440c8e3c68"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** b97c78fc-c41b-45e4-b992-785e48f7c61b<br>**Similarity:** 0.8636541927459149<br>**Text:** page_label: 3\nfile_name: Generative Adversarial Nets.pdf\nfile_path: data/Generative Adversarial Nets.pdf\nfile_type: application/pdf\nfile_size: 530482\ncreation_date: 2024-02-26\nlast_modified_date: 2024-02-26\nlast_accessed_date: 2024-02-26\ncontext_summary: Generative Adversarial Networks (GANs) involve a two-player minimax game between a generator and a discriminator, aiming to recover the data generating distribution. The training process alternates between optimizing the discriminator and the generator iteratively to prevent overfitting and ensure effective learning.\n\nx\nz\nX\nZ\nX\nZ\n. . .\nX\nZ\n(a) (b) (c) (d)\nFigure 1: Generative adversarial nets are trained by simultaneously updating the discriminative distribution\n(D, blue, dashed line) so that it discriminates between samples from the data generating distribution (black,\ndotted line)pxfrom those of the generative distribution pg(G) (green, solid line). The lower horizontal line is\nthe domain from which zis sampled, in this case uniformly. The horizontal line above is part of the domain\nofx. The upward arrows show how the mapping x=G(z)imposes the non-uniform distribution pgon\ntransformed samples. Gcontracts in regions of high density and expands in regions of low density of pg. (a)\nConsider an adversarial pair near convergence: pgis similar to pdataandDis a partially accurate classiﬁer.\n(b) In the inner loop of the algorithm Dis trained to discriminate samples from data, converging to D∗(x) =\npdata(x)\npdata(x)+pg(x). (c) After an update to G, gradient of Dhas guidedG(z)to ﬂow to regions that are more likely\nto be classiﬁed as data. (d) After several steps of training, if GandDhave enough capacity, they will reach a\npoint at which both cannot improve because pg=pdata. The discriminator is unable to differentiate between\nthe two distributions, i.e. D(x) =1\n2.\n4 Theoretical Results\nThe generator Gimplicitly deﬁnes a probability distribution pgas the distribution of the samples\nG(z)obtained when z∼pz. Therefore, we would like Algorithm 1 to converge to a good estimator\nofpdata, if given enough capacity and training time. The results of this section are done in a non-\nparametric setting, e.g. we represent a model with inﬁnite capacity by studying convergence in the\nspace of probability density functions.\nWe will show in section 4.1 that this minimax game has a global optimum for pg=pdata. We will\nthen show in section 4.2 that Algorithm 1 optimizes Eq 1, thus obtaining the desired result.\n3<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** 12ac3e22-0e62-4f9a-a154-4c504401733e<br>**Similarity:** 0.855367213245717<br>**Text:** page_label: 7\nfile_name: Generative Adversarial Nets.pdf\nfile_path: data/Generative Adversarial Nets.pdf\nfile_type: application/pdf\nfile_size: 530482\ncreation_date: 2024-02-26\nlast_modified_date: 2024-02-26\nlast_accessed_date: 2024-02-26\ncontext_summary: The context discusses generative adversarial networks (GANs) and their advantages and disadvantages compared to other generative modeling approaches. GANs use a generator and a discriminator to learn the data distribution without needing Markov chains, relying on backpropagation for training gradients.\n\nTable 2 summarizes\nthe comparison of generative adversarial nets with other generative modeling approaches.\nThe aforementioned advantages are primarily computational. Adversarial models may also gain\nsome statistical advantage from the generator network not being updated directly with data exam-\nples, but only with gradients ﬂowing through the discriminator. This means that components of the\ninput are not copied directly into the generator’s parameters. Another advantage of adversarial net-\nworks is that they can represent very sharp, even degenerate distributions, while methods based on\nMarkov chains require that the distribution be somewhat blurry in order for the chains to be able to\nmix between modes.\n7 Conclusions and future work\nThis framework admits many straightforward extensions:\n1. A conditional generative modelp(x|c)can be obtained by adding cas input to both GandD.\n2.Learned approximate inference can be performed by training an auxiliary network to predict z\ngiven x. This is similar to the inference net trained by the wake-sleep algorithm [15] but with\nthe advantage that the inference net may be trained for a ﬁxed generator net after the generator\nnet has ﬁnished training.\n7<br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example - 2**"
      ],
      "metadata": {
        "id": "RKLNUnmsdurN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = subdoc_summary_pack.run(\"What is the discriminator loss and generator loss?\")\n",
        "display(Markdown(str(response)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "WFteeEBsZcJj",
        "outputId": "f3f9719d-c150-4348-85cf-ff0fa093de4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The discriminator loss is defined as Ex∼pdata[logD∗G(x)] + Ex∼pg[log(1−D∗G(x))], while the generator loss is Ex∼pdata[log(1−D∗G(x))]."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in response.source_nodes:\n",
        "    display_source_node(n, source_length=10000, metadata_mode=\"all\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "p2bMAdr4dGP7",
        "outputId": "b59bd5c3-10e8-40f6-a70b-72aa965633c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** b97c78fc-c41b-45e4-b992-785e48f7c61b<br>**Similarity:** 0.836854163445305<br>**Text:** page_label: 3\nfile_name: Generative Adversarial Nets.pdf\nfile_path: data/Generative Adversarial Nets.pdf\nfile_type: application/pdf\nfile_size: 530482\ncreation_date: 2024-02-26\nlast_modified_date: 2024-02-26\nlast_accessed_date: 2024-02-26\ncontext_summary: Generative Adversarial Networks (GANs) involve a two-player minimax game between a generator and a discriminator, aiming to recover the data generating distribution. The training process alternates between optimizing the discriminator and the generator iteratively to prevent overfitting and ensure effective learning.\n\nx\nz\nX\nZ\nX\nZ\n. . .\nX\nZ\n(a) (b) (c) (d)\nFigure 1: Generative adversarial nets are trained by simultaneously updating the discriminative distribution\n(D, blue, dashed line) so that it discriminates between samples from the data generating distribution (black,\ndotted line)pxfrom those of the generative distribution pg(G) (green, solid line). The lower horizontal line is\nthe domain from which zis sampled, in this case uniformly. The horizontal line above is part of the domain\nofx. The upward arrows show how the mapping x=G(z)imposes the non-uniform distribution pgon\ntransformed samples. Gcontracts in regions of high density and expands in regions of low density of pg. (a)\nConsider an adversarial pair near convergence: pgis similar to pdataandDis a partially accurate classiﬁer.\n(b) In the inner loop of the algorithm Dis trained to discriminate samples from data, converging to D∗(x) =\npdata(x)\npdata(x)+pg(x). (c) After an update to G, gradient of Dhas guidedG(z)to ﬂow to regions that are more likely\nto be classiﬁed as data. (d) After several steps of training, if GandDhave enough capacity, they will reach a\npoint at which both cannot improve because pg=pdata. The discriminator is unable to differentiate between\nthe two distributions, i.e. D(x) =1\n2.\n4 Theoretical Results\nThe generator Gimplicitly deﬁnes a probability distribution pgas the distribution of the samples\nG(z)obtained when z∼pz. Therefore, we would like Algorithm 1 to converge to a good estimator\nofpdata, if given enough capacity and training time. The results of this section are done in a non-\nparametric setting, e.g. we represent a model with inﬁnite capacity by studying convergence in the\nspace of probability density functions.\nWe will show in section 4.1 that this minimax game has a global optimum for pg=pdata. We will\nthen show in section 4.2 that Algorithm 1 optimizes Eq 1, thus obtaining the desired result.\n3<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** 7dc8ad80-2e98-4102-a79e-828e765b3225<br>**Similarity:** 0.8289804466795243<br>**Text:** page_label: 4\nfile_name: Generative Adversarial Nets.pdf\nfile_path: data/Generative Adversarial Nets.pdf\nfile_type: application/pdf\nfile_size: 530482\ncreation_date: 2024-02-26\nlast_modified_date: 2024-02-26\nlast_accessed_date: 2024-02-26\ncontext_summary: The context discusses the training process of generative adversarial nets using minibatch stochastic gradient descent, where a generator and discriminator are trained simultaneously in a minimax game. The optimal discriminator for a fixed generator is derived, showing that it should output the ratio of the data generating distribution to the sum of the data generating and noise distributions.\n\nThe discriminator does not need to be deﬁned outside of Supp (pdata)∪Supp (pg),\nconcluding the proof.\nNote that the training objective for Dcan be interpreted as maximizing the log-likelihood for es-\ntimating the conditional probability P(Y=y|x), whereYindicates whether xcomes from pdata\n(withy= 1) or frompg(withy= 0). The minimax game in Eq. 1 can now be reformulated as:\nC(G) = max\nDV(G,D )\n=Ex∼pdata[logD∗\nG(x)] +Ez∼pz[log(1−D∗\nG(G(z)))] (4)\n=Ex∼pdata[logD∗\nG(x)] +Ex∼pg[log(1−D∗\nG(x))]\n=Ex∼pdata[\nlogpdata(x)\nPdata(x) +pg(x)]\n+Ex∼pg[\nlogpg(x)\npdata(x) +pg(x)]\n4<br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                            -: END : -"
      ],
      "metadata": {
        "id": "2Pn_P2nWd2ab"
      }
    }
  ]
}